// -*-c++-*-
#include "copyright.h"

#ifdef STORMM_USE_CUDA
#  if (PMENB_WARPS_PER_BLOCK == 11)
#    define PMENB_THREAD_COUNT 352
#  elif (PMENB_WARPS_PER_BLOCK == 10)
#    define PMENB_THREAD_COUNT 320
#  elif (PMENB_WARPS_PER_BLOCK ==  9)
#    define PMENB_THREAD_COUNT 288
#  elif (PMENB_WARPS_PER_BLOCK ==  8)
#    define PMENB_THREAD_COUNT 256
#  elif (PMENB_WARPS_PER_BLOCK ==  7)
#    define PMENB_THREAD_COUNT 224
#  elif (PMENB_WARPS_PER_BLOCK ==  6)
#    define PMENB_THREAD_COUNT 192
#  elif (PMENB_WARPS_PER_BLOCK ==  5)
#    define PMENB_THREAD_COUNT 160
#  elif (PMENB_WARPS_PER_BLOCK ==  4)
#    define PMENB_THREAD_COUNT 128
#  endif
#  if (__CUDA_ARCH__ >= 750 && __CUDA_ARCH__ < 800 && PMENB_THREAD_COUNT > 256)
#    undef PMENB_THREAD_COUNT
#    define PMENB_THREAD_COUNT 256
#  endif
#endif

/// \brief Compute the non-bonded energy and forces due to electrostatic and van-der Waals
///        interactions in a series of spatial decomposition cells defined by a Neutral Territory
///        layout.  Each warp will act independently for maximum granularity, while the overall
///        size of thread blocks is set to maximize thread occupancy on any given architecture.
__global__ void __launch_bounds__(PMENB_THREAD_COUNT, PMENB_BLOCK_MULTIPLICITY)
KERNEL_NAME(const SyNonbondedKit<TCALC, TCALC2> poly_nbk, const LocalExclusionMaskReader lemr,
            TilePlan tlpn, const PPIKit<TCALC, TCALC4> nrg_tab,
#ifdef CLASH_FORGIVENESS
            const TCALC clash_distance, const TCALC clash_ratio,
#endif
#ifdef COMPUTE_ENERGY
            ScoreCardWriter scw,
#endif
#ifdef DUAL_GRIDS
            CellGridWriter<TCOORD, TACC, TCOORD, TCOORD4> cgw_qq,
            CellGridWriter<TCOORD, TACC, TCOORD, TCOORD4> cgw_lj,
#else
            CellGridWriter<TCOORD, TACC, TCOORD, TCOORD4> cgw,
#endif
            MMControlKit<TCALC> ctrl) {
  
  // The coordinates and properties of receiving atoms will be passed among the individual threads,
  // while coordinates and properties of sending atoms will be held in __shared__ memory to reduce
  // register pressure.  One legacy cards with 64 kB of allocatable __shared__ memory and 32 kB of
  // additional L1, 1280 threads per SM will be engaged in five blocks of 256 threads each.  On
  // newer cards with up to 100 kB of allocatable __shared__ memory and an additional 28 kB of L1,
  // 1536 threads per SM will be engaged in six blocks.
#ifdef DUAL_GRIDS
  __shared__ bool warp_on_qq[PMENB_WARPS_PER_BLOCK];
#endif
  __shared__ uint warp_cell_counters[PMENB_WARPS_PER_BLOCK];
  __shared__ int plate_prefix_sum[ 13 * PMENB_WARPS_PER_BLOCK];
  __shared__ int tower_prefix_sum[  6 * PMENB_WARPS_PER_BLOCK];
  __shared__ int total_plate_atoms[     PMENB_WARPS_PER_BLOCK];
  __shared__ uint tower_cg_offsets[ 5 * PMENB_WARPS_PER_BLOCK];
  __shared__ uint plate_cg_offsets[12 * PMENB_WARPS_PER_BLOCK];
  __shared__ int system_idx[PMENB_WARPS_PER_BLOCK];
  __shared__ int base_tile_depth[PMENB_WARPS_PER_BLOCK];
  __shared__ int base_plan_idx[PMENB_WARPS_PER_BLOCK];
#ifdef TINY_BOX
  //__shared__ int lower_tower_starts[PMENB_WARPS_PER_BLOCK];
#endif
  __shared__ TCOORD sending_xcrd[PMENB_THREAD_COUNT];
  __shared__ TCOORD sending_ycrd[PMENB_THREAD_COUNT];
  __shared__ TCOORD sending_zcrd[PMENB_THREAD_COUNT];
  __shared__ int sending_topl_idx[PMENB_THREAD_COUNT];
  __shared__ int sending_prof_idx[PMENB_THREAD_COUNT];
  __shared__ uint sending_img_idx[PMENB_THREAD_COUNT];
  __shared__ uint recving_img_idx[PMENB_THREAD_COUNT];
  __shared__ TACC sending_xfrc[PMENB_THREAD_COUNT];
  __shared__ TACC sending_yfrc[PMENB_THREAD_COUNT];
  __shared__ TACC sending_zfrc[PMENB_THREAD_COUNT];
#ifdef LARGE_CHIP_CACHE
  // The large chip cache allows the kernel to do local accumulations of the sending atoms' forces,
  // added together without atomics.  This is the one of the lowest frequency processes in the tile
  // and therefore the overflow of such a process, which may not even be accessed, will not get
  // priority on legacy cards with only 96 kB (64 kB __shared__ plus 32 kB other L1) streaming
  // multiprocessors.
  __shared__ int sending_xfrc_ovrf[PMENB_THREAD_COUNT];
  __shared__ int sending_yfrc_ovrf[PMENB_THREAD_COUNT];
  __shared__ int sending_zfrc_ovrf[PMENB_THREAD_COUNT];
#endif
  // Set the warp and lane indices, then initialize the work unit counter
  const int warp_idx = (threadIdx.x >> warp_bits);
  const int lane_idx = (threadIdx.x & warp_bits_mask_int);
  if (lane_idx == 0) {
    warp_cell_counters[warp_idx] = (blockIdx.x * PMENB_WARPS_PER_BLOCK) + warp_idx;
  }
  SYNCWARP;

  // Loop over all work units
#ifdef DUAL_GRIDS
  while (warp_cell_counters[warp_idx] < ctrl.nt_warp_mult * (cgw_qq.total_cell_count +
                                                             cgw_lj.total_cell_count)) {
#else
  while (warp_cell_counters[warp_idx] < ctrl.nt_warp_mult * cgw.total_cell_count) {
#endif
    // Read the limits for each cell and assemble the tower and plate prefix sums.
#ifdef DUAL_GRIDS
    const bool tmp_warp_on_qq = (warp_cell_counters[warp_idx] <
                                 ctrl.nt_warp_mult * cgw_qq.total_cell_count);
    if (lane_idx == 0) {
      warp_on_qq[warp_idx] = tmp_warp_on_qq;
    }
    uint nt_idx;
    int wu_data;
    if (tmp_warp_on_qq) {
      nt_idx = warp_cell_counters[warp_idx] / ctrl.nt_warp_mult;
      wu_data = __ldcv(&cgw_qq.nt_groups[(nt_idx << warp_bits) + lane_idx]);
    }
    else {
      nt_idx = (warp_cell_counters[warp_idx] -
               (ctrl.nt_warp_mult * cgw_qq.total_cell_count)) / ctrl.nt_warp_mult;
      wu_data = __ldcv(&cgw_lj.nt_groups[((nt_idx - cgw_qq.total_cell_count) << warp_bits) +
                                         lane_idx]);
    }
    const int tower_base_pos = (warp_cell_counters[warp_idx] -
                                (ctrl.nt_warp_mult * (cgw_qq.total_cell_count + nt_idx))) *
                               warp_size_int;
#else
    const uint nt_idx = warp_cell_counters[warp_idx] / ctrl.nt_warp_mult;
    const int tower_base_pos = (warp_cell_counters[warp_idx] - (ctrl.nt_warp_mult * nt_idx)) *
                               warp_size_int;
    int wu_data = __ldcv(&cgw.nt_groups[(nt_idx << warp_bits) + lane_idx]);
#endif
#ifdef STORMM_USE_CUDA
    // The local variable wu_data is initially seeded with the cell grid cell index coming from the
    // work unit.  Later, it is repurposed to hold the current number of atoms in the tower or
    // plate cell for the purpose of computing the prefix sums.
    if (lane_idx == 29) {
      system_idx[warp_idx] = wu_data;
      wu_data = 0;
    }
    uint2 cell_contents;
    if (lane_idx < 5 || (lane_idx >= 16 && lane_idx < 28)) {
#  ifdef DUAL_GRIDS
      cell_contents = (tmp_warp_on_qq) ? cgw_qq.cell_limits[wu_data] : cgw_lj.cell_limits[wu_data];
#  else
      cell_contents = cgw.cell_limits[wu_data];
#  endif
      if (lane_idx < 5) {
        tower_cg_offsets[(warp_idx * 5) + lane_idx] = cell_contents.x;
      }
      else {
        plate_cg_offsets[(warp_idx * 12) + lane_idx - 16] = cell_contents.x;
      }
      wu_data = (cell_contents.y >> 16);
    }
    
    // Compute both prefix sums simultaneously using the layout of tower and plate cells presented
    // in the original work unit.
    const int tgx = (lane_idx & 0xf);
    wu_data += ((tgx &  1) ==  1) * __shfl_up_sync(0xffffffff, wu_data, 1, 32);
    wu_data += ((tgx &  3) ==  3) * __shfl_up_sync(0xffffffff, wu_data, 2, 32);
    wu_data += ((tgx &  7) ==  7) * __shfl_up_sync(0xffffffff, wu_data, 4, 32);
    wu_data += (tgx == 15) * __shfl_up_sync(0xffffffff, wu_data, 8, 32);
    wu_data += ((tgx &  7) == 3 && tgx > 8)  * __shfl_up_sync(0xffffffff, wu_data, 4, 32);
    wu_data += ((tgx &  3) == 1 && tgx > 4)  * __shfl_up_sync(0xffffffff, wu_data, 2, 32);
    wu_data += ((tgx &  1) == 0 && tgx >= 2) * __shfl_up_sync(0xffffffff, wu_data, 1, 32);
    wu_data = __shfl_up_sync(0xffffffff, wu_data, 1, 32);
    wu_data *= (tgx != 0);
    if (lane_idx <= 5) {
      tower_prefix_sum[(warp_idx * 6) + lane_idx] = wu_data;
    }
    else if (lane_idx >= 16 && lane_idx <= 28) {
      plate_prefix_sum[(warp_idx * 13) + lane_idx - 16] = wu_data;
      if (lane_idx == 28) {
        total_plate_atoms[warp_idx] = wu_data;
      }
    }
#endif // STORMM_USE_CUDA 
    
    // Loop over all tower atoms, taking batches of "sending" atoms, then make a nested loop over
    // all batches of atoms in the plate.
    for (int i = tower_base_pos; i < tower_prefix_sum[(warp_idx * 6) + 5];
         i += warp_size_int * ctrl.nt_warp_mult) {

      // Begin by initializing the sending atoms' force accumulators.
      sending_xfrc[threadIdx.x] = (TACC)(0);
      sending_yfrc[threadIdx.x] = (TACC)(0);
      sending_zfrc[threadIdx.x] = (TACC)(0);
#  ifdef LARGE_CHIP_CACHE
      sending_xfrc_ovrf[threadIdx.x] = 0;
      sending_yfrc_ovrf[threadIdx.x] = 0;
      sending_zfrc_ovrf[threadIdx.x] = 0;
#  else
      tlpn.xfrc_ovrf[(blockIdx.x * blockDim.x) + threadIdx.x] = 0;
      tlpn.yfrc_ovrf[(blockIdx.x * blockDim.x) + threadIdx.x] = 0;
      tlpn.zfrc_ovrf[(blockIdx.x * blockDim.x) + threadIdx.x] = 0;
#  endif      
      // If there are more than 24 atoms in this tower batch, use the full warp.  Otherwise,
      // seed the warp with groups of 16 or 8 atoms in order to complete the batch.
      uint iatom_seek;
      const int tpw_mi = tower_prefix_sum[(warp_idx * 6) + 5] - i;
      if (tpw_mi > three_quarter_warp_size_int) {

        // Take the full warp.
        iatom_seek = i + lane_idx;
        if (lane_idx == 0) {
          base_tile_depth[warp_idx] = warp_size_int;
          base_plan_idx[warp_idx] = 0;
        }
      }
      else if (tpw_mi > quarter_warp_size_int) {

        // Take up to sixteen atoms and duplicate them over the warp.  If there are more than
        // sixteen atoms left to do, the loop control variable will backtrack by 16 in order to
        // sweep up the remainder of the tower batch.
        iatom_seek = i + (lane_idx & 0xf);
        if (lane_idx == 0) {
          base_tile_depth[warp_idx] = half_warp_size_int;
          base_plan_idx[warp_idx] = 3;
        }
      }
      else {

        // Take up to eight atoms and replicate them four times over the warp.
        iatom_seek = i + (lane_idx & 0x7);
        if (lane_idx == 0) {
          base_tile_depth[warp_idx] = quarter_warp_size_int;
          base_plan_idx[warp_idx] = 6;
        }
      }

      // Because the non-bonded parameters of the sending atoms will not be stored in __shared__
      // memory on cards with less than 64kB __shared__ resources available, these variables must
      // be allocated on a per-thread basis prior to taking in data from the neighbor list.
      TCALC sending_q;
      int sending_ljidx, tmp_snd_topl_idx, tmp_snd_prof_idx;
      uint tmp_snd_img_idx;
      TCALC tmp_snd_xcrd, tmp_snd_ycrd, tmp_snd_zcrd;
      
      // Each atom can be located to within one of the five cells of the tower with a series of
      // three conditional tests (four if dual cell grids are in use).  Read critical markers
      // temporarily into registers to prevent a possible traffic jam that would happen if many
      // threads try to access the same __shared__ memory bank simultanoeously.
      const int tprfx_1 = tower_prefix_sum[(warp_idx * 6) + 1];
      const int tprfx_2 = tower_prefix_sum[(warp_idx * 6) + 2];
      const int tprfx_3 = tower_prefix_sum[(warp_idx * 6) + 3];
      const int tprfx_4 = tower_prefix_sum[(warp_idx * 6) + 4];
      if (iatom_seek >= tower_prefix_sum[(warp_idx * 6) + 5]) {
#  ifdef DUAL_GRIDS
        const TCALC fake_pos = (warp_on_qq[warp_idx]) ?
                               (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.elec_cut) :
                               (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  else
        const TCALC fake_pos = (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  endif
        tmp_snd_xcrd = fake_pos;
        tmp_snd_ycrd = -fake_pos;
        tmp_snd_zcrd = -fake_pos;
        tmp_snd_topl_idx = 0;
        tmp_snd_prof_idx = 0;
        sending_ljidx = 0;
        sending_q = (TCALC)(0.0);
        tmp_snd_img_idx = 0xffffffff;
      }
      else {
        TCOORD zc_moves;
        if (iatom_seek < tprfx_2) {
          if (iatom_seek < tprfx_1) {
            tmp_snd_img_idx = tower_cg_offsets[(warp_idx * 5)    ] + iatom_seek;
            zc_moves = (TCOORD)(-2);
          }
          else {
            tmp_snd_img_idx = tower_cg_offsets[(warp_idx * 5) + 1] + iatom_seek - tprfx_1;
            zc_moves = (TCOORD)(-1);
          }
        }
        else if (iatom_seek >= tprfx_3) {
          if (iatom_seek < tprfx_4) {
            tmp_snd_img_idx = tower_cg_offsets[(warp_idx * 5) + 3] + iatom_seek - tprfx_3;
            zc_moves = (TCOORD)(1);
          }
          else {
            tmp_snd_img_idx = tower_cg_offsets[(warp_idx * 5) + 4] + iatom_seek - tprfx_4;
            zc_moves = (TCOORD)(2);
          }
        }
        else {
          tmp_snd_img_idx = tower_cg_offsets[(warp_idx * 5) + 2] + iatom_seek - tprfx_2;
          zc_moves = (TCOORD)(0);
        }
        
        // On CUDA acrchitectures, the transform stride will be one warp's size
        const size_t cinvu_idx = system_idx[warp_idx] * warp_size_int;
#  ifdef DUAL_GRIDS
        if (warp_on_qq[warp_idx]) {
          tmp_snd_topl_idx = __ldg(&cgw_qq.nonimg_atom_idx[tmp_snd_img_idx]);
          tmp_snd_prof_idx = __ldg(&lemr.prof_idx[tmp_snd_topl_idx]);
#    ifdef TCOORD_IS_LONG
          const TCOORD4 crdq = cgw_qq.image[tmp_snd_img_idx];
#    else
          const TCOORD4 crdq = __ldg(&cgw_qq.image[tmp_snd_img_idx]);
#    endif
          tmp_snd_xcrd = crdq.x + (zc_moves * cgw_qq.system_cell_invu[cinvu_idx + 6]);
          tmp_snd_ycrd = crdq.y + (zc_moves * cgw_qq.system_cell_invu[cinvu_idx + 7]);
          tmp_snd_zcrd = crdq.z + (zc_moves * cgw_qq.system_cell_invu[cinvu_idx + 8]);
#    ifdef TCOORD_IS_REAL
          sending_q = crdq.w;
#    else
#      ifdef TCOORD_IS_LONG
          sending_q = __longlong_as_double(crdq.w);
#      else
          sending_q = __int_as_float(crdq.w);
#      endif
#    endif
        }
        else {
          tmp_snd_topl_idx = __ldg(&cgw_lj.nonimg_atom_idx[tmp_snd_img_idx]);
          tmp_snd_prof_idx = __ldg(&lemr.prof_idx[tmp_snd_topl_idx]);
#ifdef TCOORD_IS_LONG
          const TCOORD4 crdq = cgw_lj.image[tmp_snd_img_idx];
#else
          const TCOORD4 crdq = __ldg(&cgw_lj.image[tmp_snd_img_idx]);
#endif
          tmp_snd_xcrd = crdq.x + (zc_moves * cgw_lj.system_cell_invu[cinvu_idx + 6]);
          tmp_snd_ycrd = crdq.y + (zc_moves * cgw_lj.system_cell_invu[cinvu_idx + 7]);
          tmp_snd_zcrd = crdq.z + (zc_moves * cgw_lj.system_cell_invu[cinvu_idx + 8]);
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
          sending_ljidx = __double_as_longlong(crdq.w);
#      else
          sending_ljidx = __float_as_int(crdq.w);
#      endif
#    else
          sending_ljidx = crdq.w;
#    endif
        }
#  else // DUAL_GRIDS
        tmp_snd_topl_idx = __ldg(&cgw.nonimg_atom_idx[tmp_snd_img_idx]);
        tmp_snd_prof_idx = __ldg(&lemr.prof_idx[tmp_snd_topl_idx]);
#    ifdef TCOORD_IS_LONG
        const TCOORD4 crdq = cgw.image[tmp_snd_img_idx];
#    else
        const TCOORD4 crdq = __ldg(&cgw.image[tmp_snd_img_idx]);
#    endif
        tmp_snd_xcrd = crdq.x + (zc_moves * cgw.system_cell_invu[cinvu_idx + 6]);
        tmp_snd_ycrd = crdq.y + (zc_moves * cgw.system_cell_invu[cinvu_idx + 7]);
        tmp_snd_zcrd = crdq.z + (zc_moves * cgw.system_cell_invu[cinvu_idx + 8]);
        
        // A single cell grid will be expected to record both electrostatic and van-der Waals
        // parameters for the atoms.  The choice of one or two cell grids is thereby unrolling the
        // theme enumeration of the cell grid.
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
        const llint fused_param_idx = __double_as_longlong(crdq.w);
        sending_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & dp_charge_index_mask)]);
        sending_ljidx = (fused_param_idx >> dp_charge_index_bits);
#      else
        const int fused_param_idx = __float_as_int(crdq.w);
        sending_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & sp_charge_index_mask)]);
        sending_ljidx = (fused_param_idx >> sp_charge_index_bits);
#      endif
#    else
#      ifdef TCOORD_IS_LONG
        sending_q     = __ldca(&poly_nbk.q_params[(crdq.w & dp_charge_index_mask)]);
        sending_ljidx = (crdq.w >> dp_charge_index_bits);
#      else
        sending_q     = __ldca(&poly_nbk.q_params[(crdq.w & sp_charge_index_mask)]);
        sending_ljidx = (crdq.w >> sp_charge_index_bits);
#      endif
#    endif
#  endif
        // Modify the Lennard-Jones (van-der Waals) parameter index with the correct offset in
        // the synthesis compilation of tables.  Modify the charge parameter by folding in
        // Coulomb's constant at this stage.  These modifications will not need to be applied to
        // atoms of the plate.
        const int sys_idx = system_idx[warp_idx];
        sending_ljidx = (sending_ljidx * __ldca(&poly_nbk.n_lj_types[sys_idx])) +
                        __ldca(&poly_nbk.ljabc_offsets[sys_idx]);
#  ifndef TCOORD_IS_REAL
        // The __shared__ memory arrays will have converted the coordinates to TCALC type, which
        // will be either float or double.  Rescale the results according to the cell grid's
        // internal coordinate factor.
#    ifdef DUAL_GRIDS
        if (warp_on_qq[warp_idx]) {
          tmp_snd_xcrd *= cgw_qq.lpos_inv_scale;
          tmp_snd_ycrd *= cgw_qq.lpos_inv_scale;
          tmp_snd_zcrd *= cgw_qq.lpos_inv_scale;
        }
        else {
          tmp_snd_xcrd *= cgw_lj.lpos_inv_scale;
          tmp_snd_ycrd *= cgw_lj.lpos_inv_scale;
          tmp_snd_zcrd *= cgw_lj.lpos_inv_scale;
        }
#    else
        tmp_snd_xcrd *= cgw.lpos_inv_scale;
        tmp_snd_ycrd *= cgw.lpos_inv_scale;
        tmp_snd_zcrd *= cgw.lpos_inv_scale;
#    endif
#  endif
      }

      // Log the coordinates and particle indices of sending atoms in a warp-synchronous manner,
      // as these __shared__memory accesses are well coalesced but branching can be avoided.
      sending_xcrd[threadIdx.x] = tmp_snd_xcrd;
      sending_ycrd[threadIdx.x] = tmp_snd_ycrd;
      sending_zcrd[threadIdx.x] = tmp_snd_zcrd;
      sending_img_idx[threadIdx.x] = tmp_snd_img_idx;
      sending_topl_idx[threadIdx.x] = tmp_snd_topl_idx;
      sending_prof_idx[threadIdx.x] = tmp_snd_prof_idx;

      // Synchronize the warp to ensure that directives on the sending atom layout can guide
      // further plan specialization.
      SYNCWARP;

      // Loop over all atoms of the plate.
      for (int j = 0; j < total_plate_atoms[warp_idx]; j += warp_size_int) {

        // Determine any further subdivision of the tile based on the remaining number of plate
        // atoms.
        const int jbatch_size = total_plate_atoms[warp_idx] - j;
        int tile_depth, plan_idx;
        if (jbatch_size > three_quarter_warp_size_int) {
          tile_depth = base_tile_depth[warp_idx];
          plan_idx = base_plan_idx[warp_idx];
        }
        else if (jbatch_size > quarter_warp_size_int) {
          tile_depth = (base_tile_depth[warp_idx] >> 1);
          plan_idx = base_plan_idx[warp_idx] + 1;
        }
        else {
          tile_depth = (base_tile_depth[warp_idx] >> 2);
          plan_idx = base_plan_idx[warp_idx] + 2;
        }
        
        // Read atomic coordinates and properties for the receiving atoms.
        TCOORD recv_xcrd, recv_ycrd, recv_zcrd;
        TCALC recv_q;
        int recv_ljidx, recv_topl_idx;
        uint jatom_seek = j + __ldca(&tlpn.read_assign[(plan_idx * warp_size_int) + lane_idx]);
        if (jatom_seek >= total_plate_atoms[warp_idx]) {
#  ifdef DUAL_GRIDS
          const TCALC fake_pos = (warp_on_qq[warp_idx]) ?
                                 (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.elec_cut) :
                                 (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  else
          const TCALC fake_pos = (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  endif
          recv_xcrd = -fake_pos;
          recv_ycrd = -fake_pos;
          recv_zcrd = -fake_pos;
          recv_ljidx = 0;
          recv_q = (TCALC)(0.0);
          recv_topl_idx = 0;
          recving_img_idx[threadIdx.x] = 0xffffffff;
        }
        else {
          int lguess = 0;
          int hguess = 12;
          bool found;
          int mguess = ((hguess + lguess) >> 1);
          do {
            if (jatom_seek < plate_prefix_sum[(warp_idx * 13) + mguess]) {
              hguess = mguess;
              found = false;
            }
            else if (jatom_seek < plate_prefix_sum[(warp_idx * 13) + mguess + 1]) {
              found = true;
            }
            else {
              lguess = mguess;
              found = false;
            }
            mguess = ((hguess + lguess) >> 1);
          } while (! found);
          uint img_idx = plate_cg_offsets[(warp_idx * 12) + mguess] + jatom_seek -
                         plate_prefix_sum[(warp_idx * 13) + mguess];
          recving_img_idx[threadIdx.x] = img_idx;
          const int mg_row = mguess / 5;
          const TCOORD yc_moves = (TCOORD)(mg_row - 2);
          const TCOORD xc_moves = (TCOORD)(mguess - (mg_row * 5) - 2);
          const size_t cinvu_idx = system_idx[warp_idx] * warp_size_int;
#  ifdef DUAL_GRIDS
          if (warp_on_qq[warp_idx]) {
#    ifdef TCOORD_IS_LONG
            const TCOORD4 crdq = cgw_qq.image[img_idx];
#    else
            const TCOORD4 crdq = __ldg(&cgw_qq.image[img_idx]);
#    endif
            recv_topl_idx = __ldg(&cgw_qq.nonimg_atom_idx[img_idx]);
            recv_xcrd = crdq.x + (xc_moves * cgw_qq.system_cell_invu[cinvu_idx + 0]) +
                                 (yc_moves * cgw_qq.system_cell_invu[cinvu_idx + 3]);
            recv_ycrd = crdq.y + (xc_moves * cgw_qq.system_cell_invu[cinvu_idx + 1]) +
                                 (yc_moves * cgw_qq.system_cell_invu[cinvu_idx + 4]);
            recv_zcrd = crdq.z + (xc_moves * cgw_qq.system_cell_invu[cinvu_idx + 2]) +
                                 (yc_moves * cgw_qq.system_cell_invu[cinvu_idx + 5]);
#    ifdef TCOORD_IS_REAL
            recv_q = crdq.w;
#    else
#      ifdef TCOORD_IS_LONG
            recv_q = __longlong_as_double(crdq.w);
#      else
            recv_q = __int_as_float(crdq.w);
#      endif
#    endif
          }
          else {
#    ifdef TCOORD_IS_LONG
            const TCOORD4 crdq = cgw_lj.image[img_idx];
#    else
            const TCOORD4 crdq = __ldg(&cgw_lj.image[img_idx]);
#    endif
            recv_topl_idx = __ldg(&cgw_lj.nonimg_atom_idx[img_idx]);
            recv_xcrd = crdq.x + (xc_moves * cgw_lj.system_cell_invu[cinvu_idx + 0]) +
                                 (yc_moves * cgw_lj.system_cell_invu[cinvu_idx + 3]);
            recv_ycrd = crdq.y + (xc_moves * cgw_lj.system_cell_invu[cinvu_idx + 1]) +
                                 (yc_moves * cgw_lj.system_cell_invu[cinvu_idx + 4]);
            recv_zcrd = crdq.z + (xc_moves * cgw_lj.system_cell_invu[cinvu_idx + 2]) +
                                 (yc_moves * cgw_lj.system_cell_invu[cinvu_idx + 5]);
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
            recv_ljidx = __double_as_longlong(crdq.w);
#      else
            recv_ljidx = __float_as_int(crdq.w);
#      endif
#    else
            recv_ljidx = crdq.w;
#    endif
          }
#  else // DUAL_GRIDS
#    ifdef TCOORD_IS_LONG
          const TCOORD4 crdq = cgw.image[img_idx];
#    else
          const TCOORD4 crdq = __ldg(&cgw.image[img_idx]);
#    endif
          recv_topl_idx = __ldg(&cgw.nonimg_atom_idx[img_idx]);
          recv_xcrd = crdq.x + (xc_moves * cgw.system_cell_invu[cinvu_idx + 0]) +
                               (yc_moves * cgw.system_cell_invu[cinvu_idx + 3]);
          recv_ycrd = crdq.y + (xc_moves * cgw.system_cell_invu[cinvu_idx + 1]) +
                               (yc_moves * cgw.system_cell_invu[cinvu_idx + 4]);
          recv_zcrd = crdq.z + (xc_moves * cgw.system_cell_invu[cinvu_idx + 2]) +
                               (yc_moves * cgw.system_cell_invu[cinvu_idx + 5]);
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
          const llint fused_param_idx = __double_as_longlong(crdq.w);
          recv_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & dp_charge_index_mask)]);
          recv_ljidx = (fused_param_idx >> dp_charge_index_bits);
#      else
          const int fused_param_idx = __float_as_int(crdq.w);
          recv_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & sp_charge_index_mask)]);
          recv_ljidx = (fused_param_idx >> sp_charge_index_bits);
#      endif
#    else
#      ifdef TCOORD_IS_LONG
          recv_q     = __ldca(&poly_nbk.q_params[(crdq.w & dp_charge_index_mask)]);
          recv_ljidx = (crdq.w >> dp_charge_index_bits);
#      else
          recv_q     = __ldca(&poly_nbk.q_params[(crdq.w & sp_charge_index_mask)]);
          recv_ljidx = (crdq.w >> sp_charge_index_bits);
#      endif
#    endif
#  endif // DUAL_GRIDS
#  ifndef TCOORD_IS_REAL
#    ifdef DUAL_GRIDS
          if (warp_on_qq[warp_idx]) {
            recv_xcrd *= cgw_qq.lpos_inv_scale;
            recv_ycrd *= cgw_qq.lpos_inv_scale;
            recv_zcrd *= cgw_qq.lpos_inv_scale;
          }
          else {
            recv_xcrd *= cgw_lj.lpos_inv_scale;
            recv_ycrd *= cgw_lj.lpos_inv_scale;
            recv_zcrd *= cgw_lj.lpos_inv_scale;
          }
#    else
          recv_xcrd *= cgw.lpos_inv_scale;
          recv_ycrd *= cgw.lpos_inv_scale;
          recv_zcrd *= cgw.lpos_inv_scale;
#    endif
#  endif // TCOORD_IS_REAL
        }

        // Proceed to iterate the tile for the proper number of iterations.  Begin by computing
        // the interaction between the sending and receiving atoms that each thread has at hand.
        // Then, if the the loop control variable k has not yet reached the target, take the
        // receiving atom, its parameters and accumulated forces, from the next thread higher
        // (wrapping according to the warp subdivision determined by the receiving atoms).  The
        // final shuffle of receiving atoms will be coded to have the receiving atoms end up in a
        // state suitable for reducing the accumulated forces.
        TCALC send_fx = (TCALC)(0.0);
        TCALC send_fy = (TCALC)(0.0);
        TCALC send_fz = (TCALC)(0.0);
        TCALC recv_fx = (TCALC)(0.0);
        TCALC recv_fy = (TCALC)(0.0);
        TCALC recv_fz = (TCALC)(0.0);
#  ifdef DUAL_GRIDS
        if (warp_on_qq[warp_idx]) {
          for (int k = 0; k < tile_depth; k++) {

            // Determine the exclusion status.  This can produce a momentary spike in register
            // usage which is best handled prior to computing other derived quantities for the
            // particle pair.
            const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
            const uint lkp_advance = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                           recv_topl_idx, excl_mask,
                                                           lemr.aux_masks) ?
                                     nrg_tab.excl_offset : 0;
                
            // Compute the interaction between the atoms.
            const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
            const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
            const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x]; 
            const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
            const TCALC qq = recv_q * sending_q * (TCALC)(r2 < ctrl.elec_cut_sq);
#    ifdef TCALC_IS_SINGLE
            uint spl_idx = (__float_as_uint(r2) >> nrg_tab.index_shift_bits) + lkp_advance;
#    else
            uint spl_idx = (uint)(__double_as_longlong(r2) >> nrg_tab.index_shift_bits) +
                           lkp_advance;
#    endif
            const TCALC4 coef = nrg_tab.force[spl_idx];
            const TCALC invr2 = (TCALC)(1.0) / r2;
            const TCALC fmag = qq * ((r2 * coef.x) + coef.y +
                                     (((invr2 * coef.w) + coef.z) * invr2));
            send_fx += fmag * dx;
            send_fy += fmag * dy;
            send_fz += fmag * dz;
            recv_fx -= fmag * dx;
            recv_fy -= fmag * dy;
            recv_fz -= fmag * dz;

            // Move the receiving atoms, their properties, and their accumulated forces one thread
            // to the left.  Depending on the replication of the receiving atoms the index may need
            // adjustment.
            const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
            recv_xcrd = SHFL(recv_xcrd, next_thread);
            recv_ycrd = SHFL(recv_ycrd, next_thread);
            recv_zcrd = SHFL(recv_zcrd, next_thread);
            recv_fx = SHFL(recv_fx, next_thread);
            recv_fy = SHFL(recv_fy, next_thread);
            recv_fz = SHFL(recv_fz, next_thread);
            recv_q = SHFL(recv_q, next_thread);
            recv_topl_idx = SHFL(recv_topl_idx, next_thread);
          }
          storeRecvForces<TCALC, TACC>(total_plate_atoms[warp_idx] - j, lane_idx,
                                       base_plan_idx[warp_idx], tlpn.reduce_prep, recv_fx, recv_fy,
                                       recv_fz, cgw_qq.frc_scale, recving_img_idx[threadIdx.x],
                                       cgw_qq.xfrc, cgw_qq.yfrc, cgw_qq.zfrc, cgw_qq.xfrc_ovrf,
                                       cgw_qq.yfrc_ovrf, cgw_qq.zfrc_ovrf);
        }
        else {
          for (int k = 0; k < tile_depth; k++) {

            // Determine the exclusion status.  This can produce a momentary spike in register
            // usage which is best handled prior to computing other derived quantities for the
            // particle pair.
            const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
            const bool is_excl = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                       recv_topl_idx, excl_mask, lemr.aux_masks);
            const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
            const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
            const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x]; 
            const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
            const TCALC valid = (r2 < ctrl.vdw_cut_sq && (! is_excl));
            const int klj_idx = sending_ljidx + recv_ljidx;
            const TCALC lja = __ldca(&poly_nbk.lja_coeff[klj_idx]);
            const TCALC ljb = __ldca(&poly_nbk.ljb_coeff[klj_idx]);
            const TCALC invr2 = (TCALC)(1.0) / r2;
            const TCALC invr4 = invr2 * invr2;
            const TCALC fmag = (((TCALC)(6.0) * ljb) - ((TCALC)(12.0) * lja * invr4 * invr2)) *
                               invr4 * invr4 * valid;
            send_fx += fmag * dx;
            send_fy += fmag * dy;
            send_fz += fmag * dz;
            recv_fx -= fmag * dx;
            recv_fy -= fmag * dy;
            recv_fz -= fmag * dz;
            const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
            recv_xcrd = SHFL(recv_xcrd, next_thread);
            recv_ycrd = SHFL(recv_ycrd, next_thread);
            recv_zcrd = SHFL(recv_zcrd, next_thread);
            recv_fx = SHFL(recv_fx, next_thread);
            recv_fy = SHFL(recv_fy, next_thread);
            recv_fz = SHFL(recv_fz, next_thread);
            recv_ljidx = SHFL(recv_ljidx, next_thread);
            recv_topl_idx = SHFL(recv_topl_idx, next_thread);
          }
          storeRecvForces<TCALC, TACC>(total_plate_atoms[warp_idx] - j, lane_idx,
                                       base_plan_idx[warp_idx], tlpn.reduce_prep, recv_fx, recv_fy,
                                       recv_fz, cgw_lj.frc_scale, recving_img_idx[threadIdx.x],
                                       cgw_lj.xfrc, cgw_lj.yfrc, cgw_lj.zfrc, cgw_lj.xfrc_ovrf,
                                       cgw_lj.yfrc_ovrf, cgw_lj.zfrc_ovrf);
        }
#  else // DUAL_GRIDS
        for (int k = 0; k < tile_depth; k++) {
          
          // Determine the exclusion status.  This can produce a momentary spike in register
          // usage which is best handled prior to computing other derived quantities for the
          // particle pair.
          const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
          const uint lkp_advance = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                         recv_topl_idx, excl_mask,
                                                         lemr.aux_masks) ?
                                   nrg_tab.excl_offset : 0;
          const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
          const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
          const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x]; 
          const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
          const TCALC in_range = (r2 < ctrl.vdw_cut_sq);
          const TCALC qq = recv_q * sending_q * in_range;
#    ifdef TCALC_IS_SINGLE
          const uint spl_idx = (__float_as_uint(r2) >> nrg_tab.index_shift_bits) + lkp_advance;
#    else
          const uint spl_idx = (uint)(__double_as_longlong(r2) >> nrg_tab.index_shift_bits) +
                               lkp_advance;
#    endif
          const TCALC4 coef = nrg_tab.force[spl_idx];
          const TCALC invr2 = (TCALC)(1.0) / r2;
          TCALC fmag = qq * ((r2 * coef.x) + coef.y + (((invr2 * coef.w) + coef.z) * invr2));
          const int klj_idx = sending_ljidx + recv_ljidx;
          const TCALC lja = __ldca(&poly_nbk.lja_coeff[klj_idx]);
          const TCALC ljb = __ldca(&poly_nbk.ljb_coeff[klj_idx]);
          const TCALC invr4 = invr2 * invr2;

          // Use the fact that the table lookup advancement will be zero for non-excluded
          // interactions.
          fmag += (((TCALC)(6.0) * ljb) - ((TCALC)(12.0) * lja * invr4 * invr2)) *
                  invr4 * invr4 * in_range * (TCALC)(lkp_advance == 0U);

          // CHECK
#if 0
          if (fabsf(fmag) > (TCALC)(1.0e-6)) {
            const char excl_char = (lkp_advance > 0) ? 'E' : ' ';
            printf("T-Pl %4d - %4d on %2d (%2d) :: [ %9.5f %9.5f %12.4f %12.4f ] %c at %9.4f "
                   "%9.4f %9.4f [ %6u ] [ %9.4f %9.4f %9.4f  %9.4f %9.4f %9.4f ] -> %9.4f\n",
                   sending_topl_idx[threadIdx.x], recv_topl_idx, lane_idx, k, sending_q, recv_q,
                   lja, ljb, excl_char, dx, dy, dz, spl_idx, sending_xcrd[threadIdx.x],
                   sending_ycrd[threadIdx.x], sending_zcrd[threadIdx.x], recv_xcrd, recv_ycrd,
                   recv_zcrd, fmag);
          }
#endif
          // END CHECK
          
          send_fx += fmag * dx;
          send_fy += fmag * dy;
          send_fz += fmag * dz;
          recv_fx -= fmag * dx;
          recv_fy -= fmag * dy;
          recv_fz -= fmag * dz;
          const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
          recv_xcrd = SHFL(recv_xcrd, next_thread);
          recv_ycrd = SHFL(recv_ycrd, next_thread);
          recv_zcrd = SHFL(recv_zcrd, next_thread);
          recv_fx = SHFL(recv_fx, next_thread);
          recv_fy = SHFL(recv_fy, next_thread);
          recv_fz = SHFL(recv_fz, next_thread);
          recv_q = SHFL(recv_q, next_thread);
          recv_ljidx = SHFL(recv_ljidx, next_thread);
          recv_topl_idx = SHFL(recv_topl_idx, next_thread);
        }
        storeRecvForces<TCALC, TACC>(total_plate_atoms[warp_idx] - j, lane_idx,
                                     base_plan_idx[warp_idx], tlpn.reduce_prep, recv_fx, recv_fy,
                                     recv_fz, cgw.frc_scale, recving_img_idx[threadIdx.x],
                                     cgw.xfrc, cgw.yfrc, cgw.zfrc, cgw.xfrc_ovrf, cgw.yfrc_ovrf,
                                     cgw.zfrc_ovrf);
#  endif // DUAL_GRIDS

        // Hold on reducing the forces on sending atoms until all receiving atoms have been
        // processed.  Commit results to local accumulators.
#  ifdef DUAL_GRIDS
        const TCALC frc_scale = cgw_qq.frc_scale;
#  else
        const TCALC frc_scale = cgw.frc_scale;
#  endif
#  ifdef LARGE_CHIP_CACHE
        cacheSendForces(send_fx, frc_scale, sending_xfrc, sending_xfrc_ovrf);
        cacheSendForces(send_fy, frc_scale, sending_yfrc, sending_yfrc_ovrf);
        cacheSendForces(send_fz, frc_scale, sending_zfrc, sending_zfrc_ovrf);
#  else
        cacheSendForces(send_fx, frc_scale, sending_xfrc, tlpn.xfrc_ovrf);
        cacheSendForces(send_fy, frc_scale, sending_yfrc, tlpn.yfrc_ovrf);
        cacheSendForces(send_fz, frc_scale, sending_zfrc, tlpn.zfrc_ovrf);
#  endif
        // Again set the loop control variable back if there were between half and three quarters
        // of a batch available.
        const int jbatch_size_ii = total_plate_atoms[warp_idx] - j;
        if (jbatch_size_ii > half_warp_size_int && jbatch_size_ii <= three_quarter_warp_size_int) {
          j -= half_warp_size_int;
        }
      }

      // Reduce the accumulated forces on sending atoms, if required.  Commit the results to
      // global arrays by atomic operations.
      const size_t thr_idx_zu = threadIdx.x;
#  ifdef LARGE_CHIP_CACHE
      const int tmp_x_ovrf = sending_xfrc_ovrf[thr_idx_zu];
      const int tmp_y_ovrf = sending_yfrc_ovrf[thr_idx_zu];
      const int tmp_z_ovrf = sending_zfrc_ovrf[thr_idx_zu];
#  else
      const size_t gbl_cache_idx = (blockIdx.x * blockDim.x) + threadIdx.x;
      const int tmp_x_ovrf = tlpn.xfrc_ovrf[gbl_cache_idx];
      const int tmp_y_ovrf = tlpn.yfrc_ovrf[gbl_cache_idx];
      const int tmp_z_ovrf = tlpn.zfrc_ovrf[gbl_cache_idx];
#  endif
#  ifdef DUAL_GRIDS
      if (warp_on_qq[warp_idx]) {
        storeSendForces<TCALC>(lane_idx, base_plan_idx[warp_idx], sending_xfrc[thr_idx_zu],
                               sending_yfrc[thr_idx_zu], sending_zfrc[thr_idx_zu], tmp_x_ovrf,
                               tmp_y_ovrf, tmp_z_ovrf, sending_img_idx[thr_idx_zu], cgw_qq.xfrc,
                               cgw_qq.yfrc, cgw_qq.zfrc, cgw_qq.xfrc_ovrf, cgw_qq.yfrc_ovrf,
                               cgw_qq.zfrc_ovrf);
      }
      else {
        storeSendForces<TCALC>(lane_idx, base_plan_idx[warp_idx], sending_xfrc[thr_idx_zu],
                               sending_yfrc[thr_idx_zu], sending_zfrc[thr_idx_zu], tmp_x_ovrf,
                               tmp_y_ovrf, tmp_z_ovrf, sending_img_idx[thr_idx_zu], cgw_lj.xfrc,
                               cgw_lj.yfrc, cgw_lj.zfrc, cgw_lj.xfrc_ovrf, cgw_lj.yfrc_ovrf,
                               cgw_lj.zfrc_ovrf);
      }
#  else // DUAL_GRIDS
      storeSendForces<TCALC>(lane_idx, base_plan_idx[warp_idx], sending_xfrc[thr_idx_zu],
                             sending_yfrc[thr_idx_zu], sending_zfrc[thr_idx_zu], tmp_x_ovrf,
                             tmp_y_ovrf, tmp_z_ovrf, sending_img_idx[thr_idx_zu], cgw.xfrc,
                             cgw.yfrc, cgw.zfrc, cgw.xfrc_ovrf, cgw.yfrc_ovrf, cgw.zfrc_ovrf);
#  endif

      // Set the tower atom loop counter back if there are more atoms to do.  Because this will
      // only be the case if this is the final batch of warp_size_int atoms, this will not leave
      // the loop staggered the wrong way in subsequent iterations.
      const int ibatch_size = tower_prefix_sum[(warp_idx * 6) + 5] - i;
      if (ibatch_size > half_warp_size_int && ibatch_size <= three_quarter_warp_size_int) {
        i -= half_warp_size_int + (warp_size_int * (ctrl.nt_warp_mult - 1));
      }
    }
    
    // Increment the work unit counter.
    SYNCWARP;
    if (lane_idx == 0) {
      const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
      warp_cell_counters[warp_idx] = atomicAdd(&ctrl.nbwu_progress[prog_counter_idx], 1);
    }
    SYNCWARP;
#ifdef DUAL_GRIDS
  }
#else
  }
#endif

  // Set the block counters for future iterations of this kernel
  if (blockIdx.x == 0 && threadIdx.x < warp_size_int) {
    const int step_modulus = (ctrl.step & twice_warp_bits_mask_int);
    if (step_modulus == 0) {
      ctrl.nbwu_progress[threadIdx.x + warp_size_int] = gridDim.x;
    }
    if (step_modulus == warp_size_int) {
      ctrl.nbwu_progress[threadIdx.x] = gridDim.x;
    }
  }
}

// Clear the definition of the thread count per block
#ifdef STORMM_USE_CUDA
#  undef PMENB_THREAD_COUNT
#endif
