// -*-c++-*-
#include "copyright.h"

#define WARP_IDX (threadIdx.x >> warp_bits)
#define LANE_IDX (threadIdx.x & warp_bits_mask_int)

// Revise the thread count and set the storage volume in accord with the architecture
#ifdef STORMM_USE_CUDA
#  ifdef CALC_MODE_DOUBLE
#    if ORDER == 4
#      define DENSITY_SPREADING_THREADS  256
#    elif ORDER == 5
#      define DENSITY_SPREADING_THREADS  224
#    else
#      define DENSITY_SPREADING_THREADS  192
#    endif
#  else
#    ifdef ACC_MODE_DOUBLE
#      if ORDER < 6
#        if __CUDA_ARCH__ < 750
#          define DENSITY_SPREADING_THREADS  320
#        elif __CUDA_ARCH__ >= 750 && __CUDA_ARCH__ < 800
#          define DENSITY_SPREADING_THREADS  256
#        else
#          define DENSITY_SPREADING_THREADS  320
#        endif
#      else
#        if __CUDA_ARCH__ < 750
#          define DENSITY_SPREADING_THREADS  256
#        elif __CUDA_ARCH__ >= 750 && __CUDA_ARCH__ < 800
#          define DENSITY_SPREADING_THREADS  256
#        else
#          define DENSITY_SPREADING_THREADS  256
#        endif
#      endif
#    else
#      if __CUDA_ARCH__ < 750
#        if ORDER == 4
#          define DENSITY_SPREADING_THREADS  288
#        else
#          define DENSITY_SPREADING_THREADS  320
#        endif
#      elif __CUDA_ARCH__ >= 750 && __CUDA_ARCH__ < 800
#        define DENSITY_SPREADING_THREADS  256
#      else
#        define DENSITY_SPREADING_THREADS  320
#      endif
#    endif
#  endif
#  if __CUDA_ARCH__ < 700
#    ifdef SHORT_FORMAT_ACCUMULATION
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 1728
#      else
#        define STORAGE_VOLUME 3456
#      endif
#    else
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 1152
#      else
#        define STORAGE_VOLUME 1728
#      endif
#    endif
#  elif __CUDA_ARCH__ >= 700 && __CUDA_ARCH__ < 750
#    ifdef SHORT_FORMAT_ACCUMULATION
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 2688
#      else
#        define STORAGE_VOLUME 5376
#      endif
#    else
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 1792
#      else
#        define STORAGE_VOLUME 2688
#      endif
#    endif
#  elif __CUDA_ARCH__ >= 750 && __CUDA_ARCH__ < 800
#    ifdef SHORT_FORMAT_ACCUMULATION
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 1728
#      else
#        define STORAGE_VOLUME 3456
#      endif
#    else
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 1152
#      else
#        define STORAGE_VOLUME 1728
#      endif
#    endif
#  elif __CUDA_ARCH__ == 800
#    ifdef SHORT_FORMAT_ACCUMULATION
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 4896
#      else
#        define STORAGE_VOLUME 9792
#      endif
#    else
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 3264
#      else
#        define STORAGE_VOLUME 4896
#      endif
#    endif
#  else
#    ifdef SHORT_FORMAT_ACCUMULATION
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 2880
#      else
#        define STORAGE_VOLUME 5760
#      endif
#    else
#      ifdef ACC_MODE_DOUBLE
#        define STORAGE_VOLUME 1920
#      else
#        define STORAGE_VOLUME 2880
#      endif
#    endif
#  endif
#endif
__global__ void __launch_bounds__(DENSITY_SPREADING_THREADS, 4)
KERNEL_NAME(PMIGridWriter pm_wrt, MMControlKit<TCALC> ctrl,
            const CellGridReader<TMAT, void, TCALC, T4> cgr,
            const SyNonbondedKit<TCALC, TCALC2> synbk) {

  // The system's cell transformation matrix, taking Cartesian coordinates into fractional space,
  // must be stored for each work unit to align particles to the particle-mesh interaction grid.
  // The storage space is tailored to meet a thread block count of 4-5, with 256 threads per block
  // and a maximum cross section of 16 cells.  Each warp will handle its own chain of cells until
  // all cell chains have been processed.
  __shared__ volatile TCALC cell_umat[9];
#ifdef ACC_MODE_DOUBLE
  __shared__ llint primary[STORAGE_VOLUME];
#else
  __shared__ int primary[STORAGE_VOLUME];
#endif
#ifndef SHORT_FORMAT_ACCUMULATION
  __shared__ int overflow[STORAGE_VOLUME];
#endif
  __shared__ volatile int pmwu[density_mapping_wu_size];
  __shared__ volatile int warp_pos[DENSITY_SPREADING_THREADS >> warp_bits];
  __shared__ volatile int pmwu_idx, warp_progress;

  // Chain prefix sums to help locate atoms
  __shared__ volatile uint chain_prefix_sums[(DENSITY_SPREADING_THREADS >> warp_bits) *
                                             (max_shared_acc_atom_bearing_region_adim + 1)];
  __shared__ volatile uint chain_global_atom_limits[(DENSITY_SPREADING_THREADS >> warp_bits) *
                                                    (max_shared_acc_atom_bearing_region_adim + 1)];
  
  // The kernel executes work units containing three phases each:
  //
  // - Read the work unit and initialize the density mapping region.  Initialize all primary
  //   accumulators in __shared__.  On the first work unit loaded, initialize the block's
  //   exclusive space out in __global__ memory.
  // - Cycle through the following, devoting one warp to each chain in the work unit:
  //   * Compute the prefix sum for the chain
  //   * Read the atoms of each cell in chains along the unit cell A axis.  Compute B-splines and
  //     issue atomicAdd operations for any mapped points that fall within the density mapping
  //     region.
  // - Convert the integer accumulations in __shared__ to real-valued numbers, folding in the
  //   overflow accumulators in __global__ memory if any overflow was triggered.  Re-initialize
  //   the overflow accumulators if necessary.  Write the real-value results to the proper indices
  //   of the main grid in __global__ memory.
  if (WARP_IDX == 0) {
    if (threadIdx.x == 0) {
      pmwu_idx = blockIdx.x;
      warp_progress = (blockDim.x >> warp_bits);
    }
    SYNCWARP;
    if (pmwu_idx < pm_wrt.wu_count) {
      for (int pos = LANE_IDX; pos < density_mapping_wu_size; pos += warp_size_int) {
        pmwu[pos] = pm_wrt.work_units[(pmwu_idx * density_mapping_wu_size) + pos];
      }

      // Getting the same information again from L1 avoids the need for another SYNCWARP call
      const int sysid = pm_wrt.work_units[pmwu_idx * density_mapping_wu_size];
#if defined(STORMM_USE_CUDA) || defined(STORMM_USE_HIP) || defined(STORMM_USE_INTEL)
      if (LANE_IDX < 9) {
        cell_umat[LANE_IDX] = cgr.system_cell_umat[(sysid * warp_size_int) + LANE_IDX];
      }
#endif
      for (int pos = LANE_IDX; pos < (blockDim.x >> warp_bits); pos += warp_size_int) {
        warp_pos[pos] = pos;
      }
    }
  }
  else {

    // Initialize all critical elements of the __shared__ memory space
    for (int pos = threadIdx.x - warp_size_int; pos < pm_wrt.max_grid_points;
         pos += blockDim.x - warp_size_int) {
#ifdef ACC_MODE_DOUBLE
      primary[pos] = 0LL;
#else
      primary[pos] = 0;
#endif
#ifndef SHORT_FORMAT_ACCUMULATION
      overflow[pos] = 0;
#endif
    }
  }
  __syncthreads();
  
  // Loop over all work units.  The whole block advances one work unit at a time.  
  while (pmwu_idx < pm_wrt.wu_count) {

    // Each warp will now take one chain and operate on all atoms in it.  Warps will asynchronously
    // process each chain guided by a counter in __shared__.
    while (warp_pos[WARP_IDX] < pmwu[28]) {
      
      // Compute the chain index that the warp should operate upon.
      const int local_chain_cpos = (warp_pos[WARP_IDX] / pmwu[27]) / pmwu[5];
      const int local_chain_bpos = (warp_pos[WARP_IDX] / pmwu[27]) - (local_chain_cpos * pmwu[5]);
      int sys_chain_bpos = local_chain_bpos + pmwu[2];
      int sys_chain_cpos = local_chain_cpos + pmwu[3];
      sys_chain_bpos -= (sys_chain_bpos >= pmwu[23]) * pmwu[23];
      sys_chain_cpos -= (sys_chain_cpos >= pmwu[24]) * pmwu[24];
      const int chn_cell_idx_start = pmwu[25] +
                                     (((sys_chain_cpos * pmwu[23]) + sys_chain_bpos) * pmwu[22]);
      int cell_a_pos = pmwu[1] + LANE_IDX;
      cell_a_pos -= (cell_a_pos >= pmwu[22]) * pmwu[22];
      uint2 cell_lims;
      if (LANE_IDX < pmwu[4]) {
        cell_lims = cgr.cell_limits[chn_cell_idx_start + cell_a_pos];
      }
      else {
        cell_lims = { 0U, 0U };
      }
      int natom = (cell_lims.y >> 16);
      
      // The chain of cells in this work unit will not extend to the full length of the warp, on
      // any known architecture.  Use the typical exclusive prefix sum.
      EXCLUSIVE_WARP_PREFIXSUM(natom, LANE_IDX);
      const size_t prfx_idx = (WARP_IDX * (max_shared_acc_atom_bearing_region_adim + 1)) +
                              LANE_IDX;
      if (LANE_IDX < pmwu[4] + 1) {
        chain_global_atom_limits[prfx_idx] = cell_lims.x;
        chain_prefix_sums[prfx_idx] = natom;
      }
      SYNCWARP;
      int base_atom_pos = warp_size_int * (warp_pos[WARP_IDX] % pmwu[27]);
      int cell_in_chain = (WARP_IDX * (max_shared_acc_atom_bearing_region_adim + 1));
      while (BALLOT(cell_in_chain < 0) == 0) {
        const int atom_in_chain_pos = base_atom_pos + LANE_IDX;
        
        // Determine which cell the atom comes from.  For any valid atom in the chain, the loop
        // will terminate before running off the end of the valid cells.  If any atom is invalid,
        // mark that the end of the chain has been reached by marking its cell index as -1.
        TCALC frac_a, frac_b, frac_c, q;
        if (atom_in_chain_pos <
            chain_prefix_sums[(WARP_IDX *
                               (max_shared_acc_atom_bearing_region_adim + 1)) + pmwu[4]]) {
          while (atom_in_chain_pos >= chain_prefix_sums[cell_in_chain + 1]) {
            cell_in_chain++;
          }
        
          // Obtain the atom coordinates and compute the fractional position within the cell.
          // Translate the image's atom property field into a density.
          const uint img_idx = chain_global_atom_limits[cell_in_chain] + atom_in_chain_pos -
                               chain_prefix_sums[cell_in_chain];
          T4 crdq;
#ifdef TMAT_IS_LONG
          crdq = cgr.image[img_idx];
#else
          crdq = __ldca(&cgr.image[img_idx]);
#endif
#ifdef TMAT_IS_REAL
          frac_a = (cell_umat[0] * crdq.x) + (cell_umat[3] * crdq.y) + (cell_umat[6] * crdq.z);
          frac_b =                           (cell_umat[4] * crdq.y) + (cell_umat[7] * crdq.z);
          frac_c =                                                     (cell_umat[8] * crdq.z);

          // Resolve the density quantity to release the 16- or 32-byte crdq object from registers.
          switch (pm_wrt.theme) {
          case NonbondedTheme::ELECTROSTATIC:
            switch (cgr.theme) {
            case NonbondedTheme::ELECTROSTATIC:
              q = crdq.w;
              break;
            case NonbondedTheme::VAN_DER_WAALS:
              break;
            case NonbondedTheme::ALL:
              {
#  ifdef TMAT_IS_LONG
                const llint qprm_as_lli = __double_as_longlong(crdq.w);
                const int qprm_idx = (qprm_as_lli & dp_charge_index_mask);
#  else
                const int qprm_as_i = __float_as_int(crdq.w);
                const int qprm_idx = (qprm_as_i & sp_charge_index_mask);
#  endif
                q = synbk.q_params[qprm_idx];
              }
              break;
            }
            break;
          case NonbondedTheme::VAN_DER_WAALS:
            switch (cgr.theme) {
            case NonbondedTheme::ELECTROSTATIC:
              break;
            case NonbondedTheme::VAN_DER_WAALS:
              {
#  ifdef TMAT_IS_LONG
                const int tlj_idx = __double_as_longlong(crdq.w);
#  else
                const int tlj_idx = __float_as_int(crdq.w);
#  endif
                const TCALC2 tlj_ab = synbk.ljab_coeff[synbk.ljabc_offsets[pmwu[0]] +
                                                       ((synbk.n_lj_types[pmwu[0]] + 1) *
                                                        tlj_idx)];
                q = tlj_ab.y;
#  ifdef TCALC_IS_DOUBLE
                q = sqrt((TCALC)(0.25) * q);
#  else
                q = sqrtf((TCALC)(0.25) * q);
#  endif
              }
              break;
            case NonbondedTheme::ALL:
              {
#  ifdef TMAT_IS_LONG
                const llint ljidx_as_lli = __double_as_longlong(crdq.w);
                const int tlj_idx = (ljidx_as_lli >> dp_charge_index_bits);
#  else
                const int ljidx_as_i = __float_as_int(crdq.w);
                const int tlj_idx = (ljidx_as_i >> sp_charge_index_bits);
#  endif
                const TCALC2 tlj_ab = synbk.ljab_coeff[synbk.ljabc_offsets[pmwu[0]] +
                                                       ((synbk.n_lj_types[pmwu[0]] + 1) *
                                                        tlj_idx)];
                q = tlj_ab.y;
#  ifdef TCALC_IS_DOUBLE
                q = sqrt((TCALC)(0.25) * q);
#  else
                q = sqrtf((TCALC)(0.25) * q);
#  endif
              }
              break;
            }
            break;
          case NonbondedTheme::ALL:
            break;
          }
#else // TMAT_IS_REAL
          const TCALC crd_x = (TCALC)(crdq.x) * cgr.inv_lpos_scale;
          const TCALC crd_y = (TCALC)(crdq.y) * cgr.inv_lpos_scale;
          const TCALC crd_z = (TCALC)(crdq.z) * cgr.inv_lpos_scale;
          frac_a = (cell_umat[0] * crd_x) + (cell_umat[3] * crd_y) + (cell_umat[6] * crd_z);
          frac_b =                          (cell_umat[4] * crd_y) + (cell_umat[7] * crd_z);
          frac_c =                                                   (cell_umat[8] * crd_z);

          // Resolve the density quantity to release the 16- or 32-byte crdq object from registers.
          switch (pm_wrt.theme) {
          case NonbondedTheme::ELECTROSTATIC:
            switch (cgr.theme) {
            case NonbondedTheme::ELECTROSTATIC:
#  ifdef TMAT_IS_LONG
              q = __longlong_as_double(crdq.w);
#  else
              q = __int_as_float(crdq.w);
#  endif
              break;
            case NonbondedTheme::VAN_DER_WAALS:
              break;
            case NonbondedTheme::ALL:
              {
#  ifdef TMAT_IS_LONG
                const int qprm_idx = (crdq.w & dp_charge_index_mask);
#  else
                const int qprm_idx = (crdq.w & sp_charge_index_mask);
#  endif
                q = synbk.q_params[qprm_idx];
              }
              break;
            }
            break;
          case NonbondedTheme::VAN_DER_WAALS:
            switch (cgr.theme) {
            case NonbondedTheme::ELECTROSTATIC:
              break;
            case NonbondedTheme::VAN_DER_WAALS:
              {
                const int tlj_idx = crdq.w;
                const TCALC2 tlj_ab = synbk.ljab_coeff[synbk.ljabc_offsets[pmwu[0]] +
                                                       ((synbk.n_lj_types[pmwu[0]] + 1) *
                                                        tlj_idx)];
                q = tlj_ab.y;
                                   
#  ifdef TCALC_IS_DOUBLE
                q = sqrt((TCALC)(0.25) * q);
#  else
                q = sqrtf((TCALC)(0.25) * q);
#  endif
              }
              break;
            case NonbondedTheme::ALL:
              {
#  ifdef TMAT_IS_LONG
                const int tlj_idx = (crdq.w >> dp_charge_index_bits);
#  else
                const int tlj_idx = (crdq.w >> sp_charge_index_bits);
#  endif
                const TCALC2 tlj_ab = synbk.ljab_coeff[synbk.ljabc_offsets[pmwu[0]] +
                                                       ((synbk.n_lj_types[pmwu[0]] + 1) *
                                                        tlj_idx)];
                q = tlj_ab.y;
#  ifdef TCALC_IS_DOUBLE
                q = sqrt((TCALC)(0.25) * q);
#  else
                q = sqrtf((TCALC)(0.25) * q);
#  endif
              }
              break;
            }
            break;
          case NonbondedTheme::ALL:
            break;
          }
#endif // TMAT_IS_REAL

        }
        else {
          frac_a = (TCALC)(0.0);
          frac_b = (TCALC)(0.0);
          frac_c = (TCALC)(0.0);
          q      = (TCALC)(0.0);
          cell_in_chain = -1;
        }

        // Only proceed if there is at least one valid atom.
        if (BALLOT(cell_in_chain >= 0)) {
        
          // Determine the starting indices for the atom's density to map onto the local grid.
          int ida, idb, idc;
#ifdef TCALC_IS_DOUBLE
          const TCALC da = imageLocalFraction(frac_a, cgr.mesh_ticks, &ida);
          const TCALC db = imageLocalFraction(frac_b, cgr.mesh_ticks, &idb);
          const TCALC dc = imageLocalFraction(frac_c, cgr.mesh_ticks, &idc);
#else
          const TCALC da = imageLocalFractionf(frac_a, cgr.mesh_ticks, &ida);
          const TCALC db = imageLocalFractionf(frac_b, cgr.mesh_ticks, &idb);
          const TCALC dc = imageLocalFractionf(frac_c, cgr.mesh_ticks, &idc);
#endif
          const int local_grid_offset = pmwu[4] - pmwu[10];
          ida += (cell_in_chain - (WARP_IDX * (max_shared_acc_atom_bearing_region_adim + 1)) -
                  local_grid_offset) * cgr.mesh_ticks;
          idb += (local_chain_bpos - local_grid_offset) * cgr.mesh_ticks;
          idc += (local_chain_cpos - local_grid_offset) * cgr.mesh_ticks;

          // Each warp cycles through this phase as a whole, but not all threads will have valid
          // atoms with density to contribute.  An atom that does have valid density to contribute
          // may not contribute to all points of its stencil, if the stencil does not lie entirely
          // within the local grid mapping region.  Roll invalid atoms into the check that will
          // catch off-grid stencil points by blasting their indices into uncharted territory.
          idb += ((cell_in_chain < 0) << 16);
        
          // Compute B-spline coefficients given the deltas.
          TCALC bspln_a_knots[ORDER], bspln_b_knots[ORDER], bspln_c_knots[ORDER];
#if ORDER == 4
          devcBSpline4(da, bspln_a_knots);
          devcBSpline4(db, bspln_b_knots);
          devcBSpline4(dc, bspln_c_knots);
#elif ORDER == 5
          devcBSpline5(da, bspln_a_knots);
          devcBSpline5(db, bspln_b_knots);
          devcBSpline5(dc, bspln_c_knots);
#elif ORDER == 6
          devcBSpline6(da, bspln_a_knots);
          devcBSpline6(db, bspln_b_knots);
          devcBSpline6(dc, bspln_c_knots);
#endif
          q *= pm_wrt.shacc_fp_scale;
          for (int i = 0; i < ORDER; i++) {
            bspln_a_knots[i] *= q;
          }

          // Loop over all grid points, issuing atomic additions to the particle-mesh interaction
          // grid.
          const int gmap_a_limit = pmwu[10] * cgr.mesh_ticks;
          const int gmap_b_limit = pmwu[11] * cgr.mesh_ticks;
          const int gmap_c_limit = pmwu[12] * cgr.mesh_ticks;
#if ORDER == 4
#pragma unroll 4
#elif ORDER == 5
#pragma unroll 5
#elif ORDER == 6
#pragma unroll 6
#endif
          for (int k = 0; k < ORDER; k++) {
            for (int j = 0; j < ORDER; j++) {
              const int base_kj_idx = ((((k + idc) * gmap_b_limit) + j + idb) * gmap_a_limit);
              const bool pass_kj = (j + idb >= 0 && j + idb < gmap_b_limit &&
                                    k + idc >= 0 && k + idc < gmap_c_limit);
              const TCALC prod_kj = (pass_kj) ? bspln_b_knots[j] * bspln_c_knots[k] : (TCALC)(0.0);
#if ORDER == 4
#pragma unroll 4
#elif ORDER == 5
#pragma unroll 5
#elif ORDER == 6
#pragma unroll 6
#endif
              for (int i = 0; i < ORDER; i++) {
                if (i + ida >= 0 && i + ida < gmap_a_limit && prod_kj) {
#ifdef SHORT_FORMAT_ACCUMULATION
#  ifdef ACC_MODE_DOUBLE
                  const llint contrib = bspln_a_knots[i] * prod_kj;
                  atomicAdd((ullint*)&primary[base_kj_idx + i + ida], (ullint)(contrib));
#  else
                  const int contrib = bspln_a_knots[i] * prod_kj;
                  atomicAdd(&primary[base_kj_idx + i + ida], contrib);
#  endif
#else
                  atomicSplit(bspln_a_knots[i] * prod_kj, base_kj_idx + i + ida, primary,
                              overflow);
#endif
                }
              }
            }
          }
        }

        // Advance to the next batch of atoms.
        base_atom_pos += pmwu[29];
      }

      // Advance to the next chain.
      SYNCWARP;
      if (LANE_IDX == 0) {
        warp_pos[WARP_IDX] = atomicAdd((int*)&warp_progress, 1);
      }
      SYNCWARP;
    }
    __syncthreads();
    
    // Convert the results to real-valued numbers and write them to the density grid in global
    // memory.  While this will involve a global read of a lot of data that has likely left L1, the
    // data may still be preserved in L2.
    for (int pos = threadIdx.x; pos < pmwu[17]; pos += blockDim.x) {
      
      // Compute the position in the work unit's grid-mapping region.
      const int local_a_len = pmwu[10] * cgr.mesh_ticks;
      const int local_ab_slab = local_a_len * pmwu[11] * cgr.mesh_ticks;
      const int local_cidx = pos / local_ab_slab;
      const int local_bidx = (pos - (local_cidx * local_ab_slab)) / local_a_len;
      const int local_aidx = pos - ((local_cidx * local_ab_slab) + (local_bidx * local_a_len));

      // Compute the position in the overall particle-mesh interaction grid.
      int gbl_aidx = (pmwu[7] * cgr.mesh_ticks) + local_aidx;
      int gbl_bidx = (pmwu[8] * cgr.mesh_ticks) + local_bidx;
      int gbl_cidx = (pmwu[9] * cgr.mesh_ticks) + local_cidx;
      gbl_aidx -= (gbl_aidx >= pmwu[18]) * pmwu[18];
      gbl_bidx -= (gbl_bidx >= pmwu[19]) * pmwu[19];
      gbl_cidx -= (gbl_cidx >= pmwu[20]) * pmwu[20];
      uint gbl_pos;
      switch (pm_wrt.fftm) {
      case FFTMode::IN_PLACE:
        {
          const uint padded_xdim = 2 * ((pmwu[18] / 2) + 1);
          gbl_pos = (((gbl_cidx * pmwu[19]) + gbl_bidx) * padded_xdim) + gbl_aidx;
        }
        break;
      case FFTMode::OUT_OF_PLACE:
        gbl_pos = (((gbl_cidx * pmwu[19]) + gbl_bidx) * pmwu[18]) + gbl_aidx;
        break;
      }

      // Read the primary accumulator in __shared__ and the overflow which received additional
      // atomic contributions in L2.  Use write-through stores, as these values are final and
      // should not pollute L1 or L2.  It is not useful to re-initialize the __global__ memory
      // space at this stage, as it is not known how extensive the next work unit will be.
#ifdef ACC_MODE_DOUBLE
#  ifndef SHORT_FORMAT_ACCUMULATION
      const int95_t ival = { primary[pos], overflow[pos] };
      const double qval = splitFPToReal(ival) / pm_wrt.shacc_fp_scale;
#  else
      const double qval = (double)(primary[pos]) / pm_wrt.shacc_fp_scale;
#  endif
      __stwt(&pm_wrt.ddata[gbl_pos + pmwu[21]], qval);
#else
#  ifndef SHORT_FORMAT_ACCUMULATION
      const int2 ival = { primary[pos], overflow[pos] };
      const float qval = splitFPToReal(ival) / pm_wrt.shacc_fp_scale;
#  else
      const float qval = (float)(primary[pos]) / pm_wrt.shacc_fp_scale;
#  endif
      __stwt(&pm_wrt.fdata[gbl_pos + pmwu[21]], qval);
#endif
    }
    __syncthreads();

    // Advance to the next work unit.
    if (WARP_IDX == 0) {
      if (threadIdx.x == 0) {
        const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
        pmwu_idx = atomicAdd(&ctrl.pmewu_progress[prog_counter_idx], 1);
        warp_progress = (blockDim.x >> warp_bits);
      }
      SYNCWARP;
      if (pmwu_idx < pm_wrt.wu_count) {
        for (int pos = LANE_IDX; pos < density_mapping_wu_size; pos += warp_size_int) {
          pmwu[pos] = pm_wrt.work_units[(pmwu_idx * density_mapping_wu_size) + pos];
        }

        // Getting the same information again from L1 avoids the need for another SYNCWARP call
        const int sysid = pm_wrt.work_units[pmwu_idx * density_mapping_wu_size];
#if defined(STORMM_USE_CUDA) || defined(STORMM_USE_HIP) || defined(STORMM_USE_INTEL)
        if (LANE_IDX < 9) {
          cell_umat[LANE_IDX] = cgr.system_cell_umat[(sysid * warp_size_int) + LANE_IDX];
        }
#endif
        for (int pos = LANE_IDX; pos < (blockDim.x >> warp_bits); pos += warp_size_int) {
          warp_pos[pos] = pos;
        }
      }
    }
    else {

      // Reset all critical elements of the __shared__ and __global__ memory spaces.
      for (int pos = threadIdx.x - warp_size_int; pos < pm_wrt.max_grid_points;
           pos += blockDim.x - warp_size_int) {
#ifdef ACC_MODE_DOUBLE
        primary[pos] = 0LL;
#else
        primary[pos] = 0;
#endif
#ifndef SHORT_FORMAT_ACCUMULATION
        overflow[pos] = 0;
#endif
      }
    }
    __syncthreads();
  }

  // Set the block counters for future iterations of this kernel
  if (blockIdx.x == 0 && threadIdx.x < warp_size_int) {
    const int step_modulus = (ctrl.step & twice_warp_bits_mask_int);
    if (step_modulus == 0) {
      ctrl.pmewu_progress[threadIdx.x + warp_size_int] = gridDim.x;
    }
    if (step_modulus == warp_size_int) {
      ctrl.pmewu_progress[threadIdx.x] = gridDim.x;
    }
  }
}

// Release various macro definitions
#undef WARP_IDX
#undef LANE_IDX
#ifdef STORMM_USE_CUDA
#  undef DENSITY_SPREADING_THREADS
#  undef STORAGE_VOLUME
#endif
