// -*-c++-*-
#include "copyright.h"

#ifdef STORMM_USE_CUDA
#  if (PMENB_WARPS_PER_BLOCK == 11)
#    define PMENB_THREAD_COUNT 352
#  elif (PMENB_WARPS_PER_BLOCK == 10)
#    define PMENB_THREAD_COUNT 320
#  elif (PMENB_WARPS_PER_BLOCK ==  9)
#    define PMENB_THREAD_COUNT 288
#  elif (PMENB_WARPS_PER_BLOCK ==  8)
#    define PMENB_THREAD_COUNT 256
#  elif (PMENB_WARPS_PER_BLOCK ==  7)
#    define PMENB_THREAD_COUNT 224
#  elif (PMENB_WARPS_PER_BLOCK ==  6)
#    define PMENB_THREAD_COUNT 192
#  elif (PMENB_WARPS_PER_BLOCK ==  5)
#    define PMENB_THREAD_COUNT 160
#  elif (PMENB_WARPS_PER_BLOCK ==  4)
#    define PMENB_THREAD_COUNT 128
#  endif
#  if (__CUDA_ARCH__ >= 750 && __CUDA_ARCH__ < 800 && PMENB_THREAD_COUNT > 256)
#    undef PMENB_THREAD_COUNT
#    define PMENB_THREAD_COUNT 256
#  endif
#endif

/// \brief Compute the non-bonded energy and forces due to electrostatic and van-der Waals
///        interactions in a series of spatial decomposition cells defined by a Neutral Territory
///        layout.  Each warp will act independently for maximum granularity, while the overall
///        size of thread blocks is set to maximize thread occupancy on any given architecture.
__global__ void __launch_bounds__(PMENB_THREAD_COUNT, PMENB_BLOCK_MULTIPLICITY)
KERNEL_NAME(const SyNonbondedKit<TCALC, TCALC2> poly_nbk, const LocalExclusionMaskReader lemr,
            TilePlan tlpn, const PPIKit<TCALC, TCALC4> nrg_tab,
#ifdef CLASH_FORGIVENESS
            const TCALC clash_distance, const TCALC clash_ratio,
#endif
#ifdef COMPUTE_ENERGY
            ScoreCardWriter scw,
#endif
#ifdef DUAL_GRIDS
            CellGridWriter<TCOORD, TACC, TCOORD, TCOORD4> cgw_qq,
            CellGridWriter<TCOORD, TACC, TCOORD, TCOORD4> cgw_lj,
#else
            CellGridWriter<TCOORD, TACC, TCOORD, TCOORD4> cgw,
#endif
            MMControlKit<TCALC> ctrl) {

  // The coordinates and properties of receiving atoms will be passed among the individual threads,
  // while coordinates and properties of sending atoms will be held in __shared__ memory to reduce
  // register pressure.  One legacy cards with 64 kB of allocatable __shared__ memory and 32 kB of
  // additional L1, 1280 threads per SM will be engaged in five blocks of 256 threads each.  On
  // newer cards with up to 100 kB of allocatable __shared__ memory and an additional 28 kB of L1,
  // 1536 threads per SM will be engaged in six blocks.
#ifdef DUAL_GRIDS
  __shared__ bool warp_on_qq[PMENB_WARPS_PER_BLOCK];
#endif
  __shared__ uint warp_cell_counters[PMENB_WARPS_PER_BLOCK];
  __shared__ int tower_prefix_sum[  6 * PMENB_WARPS_PER_BLOCK];
  __shared__ uint tower_cg_offsets[ 5 * PMENB_WARPS_PER_BLOCK];
  __shared__ uint center_base_pos[PMENB_WARPS_PER_BLOCK];
  __shared__ uint center_max_pos[PMENB_WARPS_PER_BLOCK];
  __shared__ int system_idx[PMENB_WARPS_PER_BLOCK];
  __shared__ int base_plan_idx[PMENB_WARPS_PER_BLOCK];
  __shared__ int self_tile_depth[PMENB_WARPS_PER_BLOCK];
  __shared__ int self_plan_idx[PMENB_WARPS_PER_BLOCK];
#ifdef TINY_BOX
  //__shared__ int lower_tower_starts[PMENB_WARPS_PER_BLOCK];
#endif
  __shared__ TCOORD sending_xcrd[PMENB_THREAD_COUNT];
  __shared__ TCOORD sending_ycrd[PMENB_THREAD_COUNT];
  __shared__ TCOORD sending_zcrd[PMENB_THREAD_COUNT];
  __shared__ int sending_topl_idx[PMENB_THREAD_COUNT];
  __shared__ int sending_prof_idx[PMENB_THREAD_COUNT];
  __shared__ uint sending_img_idx[PMENB_THREAD_COUNT];
  __shared__ uint recving_img_idx[PMENB_THREAD_COUNT];
  __shared__ TACC sending_xfrc[PMENB_THREAD_COUNT];
  __shared__ TACC sending_yfrc[PMENB_THREAD_COUNT];
  __shared__ TACC sending_zfrc[PMENB_THREAD_COUNT];
#ifdef LARGE_CHIP_CACHE
  // The large chip cache allows the kernel to do local accumulations of the sending atoms' forces,
  // added together without atomics.  This is the one of the lowest frequency processes in the tile
  // and therefore the overflow of such a process, which may not even be accessed, will not get
  // priority on legacy cards with only 96 kB (64 kB __shared__ plus 32 kB other L1) streaming
  // multiprocessors.
  __shared__ int sending_xfrc_ovrf[PMENB_THREAD_COUNT];
  __shared__ int sending_yfrc_ovrf[PMENB_THREAD_COUNT];
  __shared__ int sending_zfrc_ovrf[PMENB_THREAD_COUNT];
#endif

  // Set the warp and lane indices, then initialize the work unit counter
  const int warp_idx = (threadIdx.x >> warp_bits);
  const int lane_idx = (threadIdx.x & warp_bits_mask_int);
  if (lane_idx == 0) {
    warp_cell_counters[warp_idx] = (blockIdx.x * PMENB_WARPS_PER_BLOCK) + warp_idx;
  }
  SYNCWARP;

  // Loop over all work units, completing the central cell's interactions with itself and with
  // the lower tower.
#ifdef DUAL_GRIDS
  while (warp_cell_counters[warp_idx] < (cgw_qq.total_cell_count + cgw_lj.total_cell_count)) {
#else
  while (warp_cell_counters[warp_idx] < cgw.total_cell_count) {
#endif
    
    // Read the limits for each cell and assemble the tower and plate prefix sums.
#ifdef DUAL_GRIDS
    const bool tmp_warp_on_qq = (warp_cell_counters[warp_idx] < cgw_qq.total_cell_count);
    if (lane_idx == 0) {
      warp_on_qq[warp_idx] = tmp_warp_on_qq;
    }
    uint nt_idx;
    int wu_data;
    if (tmp_warp_on_qq) {
      nt_idx = warp_cell_counters[warp_idx];
      wu_data = __ldcv(&cgw_qq.nt_groups[(nt_idx << warp_bits) + lane_idx]);
    }
    else {
      nt_idx = warp_cell_counters[warp_idx] - cgw_qq.total_cell_count;
      wu_data = __ldcv(&cgw_lj.nt_groups[((nt_idx - cgw_qq.total_cell_count) << warp_bits) +
                                         lane_idx]);
    }
#else
    const uint nt_idx = warp_cell_counters[warp_idx];
    int wu_data = __ldcv(&cgw.nt_groups[(nt_idx << warp_bits) + lane_idx]);
#endif
#ifdef STORMM_USE_CUDA
    // The local variable wu_data is initially seeded with the cell grid cell index coming from the
    // work unit.  Later, it is repurposed to hold the current number of atoms in the tower or
    // plate cell for the purpose of computing the prefix sums.
    if (lane_idx == 29) {
      system_idx[warp_idx] = wu_data;
      wu_data = 0;
    }
    if (lane_idx < 5) {
#  ifdef DUAL_GRIDS
      const uint2 cell_contents = (tmp_warp_on_qq) ?
                                  cgw_qq.cell_limits[wu_data] : cgw_lj.cell_limits[wu_data];
#  else
      const uint2 cell_contents = cgw.cell_limits[wu_data];
#  endif
      if (lane_idx < 5) {
        tower_cg_offsets[(warp_idx * 5) + lane_idx] = cell_contents.x;
      }
      wu_data = (cell_contents.y >> 16);
      if (lane_idx == 2) {
        center_base_pos[warp_idx] = cell_contents.x;
        center_max_pos[warp_idx] = cell_contents.x + wu_data;
      }
    }
    
    // Compute both prefix sums simultaneously using the layout of tower and plate cells presented
    // in the original work unit.
    const int tgx = (lane_idx & 0xf);
    wu_data += ((tgx &  1) ==  1) * __shfl_up_sync(0xffffffff, wu_data, 1, 32);
    wu_data += ((tgx &  3) ==  3) * __shfl_up_sync(0xffffffff, wu_data, 2, 32);
    wu_data += ((tgx &  7) ==  7) * __shfl_up_sync(0xffffffff, wu_data, 4, 32);
    wu_data += (tgx == 15) * __shfl_up_sync(0xffffffff, wu_data, 8, 32);
    wu_data += ((tgx &  7) == 3 && tgx > 8)  * __shfl_up_sync(0xffffffff, wu_data, 4, 32);
    wu_data += ((tgx &  3) == 1 && tgx > 4)  * __shfl_up_sync(0xffffffff, wu_data, 2, 32);
    wu_data += ((tgx &  1) == 0 && tgx >= 2) * __shfl_up_sync(0xffffffff, wu_data, 1, 32);
    wu_data = __shfl_up_sync(0xffffffff, wu_data, 1, 32);
    wu_data *= (tgx != 0);
    if (lane_idx <= 5) {
      tower_prefix_sum[(warp_idx * 6) + lane_idx] = wu_data;
    }
#endif // STORMM_USE_CUDA
    
    // Loop over all atoms in the central cell, first completing a self-interaction tile, then
    // completing tiles with any other atoms in the central cell, and finally completing tiles
    // with all atoms in the lower part of the tower.
    if (lane_idx == 0) {
#  ifdef DUAL_GRIDS
      if (warp_on_qq[warp_idx]) {
        const uint center_idx = warp_cell_counters[warp_idx];
        const uint2 center_lims = cgw_qq.cell_limits[center_idx];
        center_base_pos[warp_idx] = center_lims.x;
        center_max_pos[warp_idx] = center_lims.x + (center_lims.y >> 16);
      }
      else {
        const uint center_idx = warp_cell_counters[warp_idx] - cgw_qq.total_cell_count;
        const uint2 center_lims = cgw_lj.cell_limits[center_idx];
        center_base_pos[warp_idx] = center_lims.x;
        center_max_pos[warp_idx] = center_lims.x + (center_lims.y >> 16);
      }
#  else
      const uint center_idx = warp_cell_counters[warp_idx];
      const uint2 center_lims = cgw.cell_limits[center_idx];
      center_base_pos[warp_idx] = center_lims.x;
      center_max_pos[warp_idx] = center_lims.x + (center_lims.y >> 16);
#  endif
    }
    SYNCWARP;
    for (uint i = center_base_pos[warp_idx]; i < center_max_pos[warp_idx]; i += warp_size_uint) {

      // Begin by initializing the sending atoms' force accumulators.
      sending_xfrc[threadIdx.x] = (TACC)(0);
      sending_yfrc[threadIdx.x] = (TACC)(0);
      sending_zfrc[threadIdx.x] = (TACC)(0);
#  ifdef LARGE_CHIP_CACHE
      sending_xfrc_ovrf[threadIdx.x] = 0;
      sending_yfrc_ovrf[threadIdx.x] = 0;
      sending_zfrc_ovrf[threadIdx.x] = 0;
#  else
      tlpn.xfrc_ovrf[(blockIdx.x * blockDim.x) + threadIdx.x] = 0;
      tlpn.yfrc_ovrf[(blockIdx.x * blockDim.x) + threadIdx.x] = 0;
      tlpn.zfrc_ovrf[(blockIdx.x * blockDim.x) + threadIdx.x] = 0;
#  endif
      uint iatom_seek;
      if (center_max_pos[warp_idx] - i > half_warp_size_int) {

        // Take the entire warp, with no atom replication.
        iatom_seek = i + lane_idx;
        if (lane_idx == 0) {
          self_tile_depth[warp_idx] = warp_size_int;
          self_plan_idx[warp_idx] = 0;
        }
      }
      else if (center_max_pos[warp_idx] - i > quarter_warp_size_int) {

        // Replicate the atoms two-fold.
        iatom_seek = i + (lane_idx & 0xf);
        if (lane_idx == 0) {
          self_tile_depth[warp_idx] = half_warp_size_int;
          self_plan_idx[warp_idx] = 3;
        }
      }
      else {

        // Replicate the atoms four-fold.
        iatom_seek = i + (lane_idx & 0x7);
        if (lane_idx == 0) {
          self_tile_depth[warp_idx] = quarter_warp_size_int;
          self_plan_idx[warp_idx] = 6;
        }
      }

      // Again allocate and assign parameters for each of the "sending" atoms.  This is a
      // simpler process than when the entire tower was in play.
      TCALC sending_q;
      int sending_ljidx, tmp_snd_topl_idx, tmp_snd_prof_idx;
      uint tmp_snd_img_idx;
      TCALC tmp_snd_xcrd, tmp_snd_ycrd, tmp_snd_zcrd;
      if (iatom_seek >= center_max_pos[warp_idx]) {
#  ifdef DUAL_GRIDS
        const TCALC fake_pos = (warp_on_qq[warp_idx]) ?
                               (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.elec_cut) :
                               (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  else
        const TCALC fake_pos = (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  endif
        tmp_snd_xcrd = fake_pos;
        tmp_snd_ycrd = -fake_pos;
        tmp_snd_zcrd = -fake_pos;
        tmp_snd_topl_idx = 0;
        tmp_snd_prof_idx = 0;
        sending_ljidx = 0;
        sending_q = (TCALC)(0.0);
        tmp_snd_img_idx = 0xffffffff;
      }
      else {
        tmp_snd_img_idx = iatom_seek;
#  ifdef DUAL_GRIDS
        if (warp_on_qq[warp_idx]) {
          tmp_snd_topl_idx = __ldg(&cgw_qq.nonimg_atom_idx[tmp_snd_img_idx]);
          tmp_snd_prof_idx = __ldg(&lemr.prof_idx[tmp_snd_topl_idx]);
#    ifdef TCOORD_IS_LONG
          const TCOORD4 crdq = cgw_qq.image[tmp_snd_img_idx];
#    else
          const TCOORD4 crdq = __ldg(&cgw_qq.image[tmp_snd_img_idx]);
#    endif
          tmp_snd_xcrd = crdq.x;
          tmp_snd_ycrd = crdq.y;
          tmp_snd_zcrd = crdq.z;
#    ifdef TCOORD_IS_REAL
          sending_q = crdq.w;
#    else
#      ifdef TCOORD_IS_LONG
          sending_q = __longlong_as_double(crdq.w);
#      else
          sending_q = __int_as_float(crdq.w);
#      endif
#    endif
        }
        else {
          tmp_snd_topl_idx = __ldg(&cgw_lj.nonimg_atom_idx[tmp_snd_img_idx]);
          tmp_snd_prof_idx = __ldg(&lemr.prof_idx[tmp_snd_topl_idx]);
#    ifdef TCOORD_IS_LONG
          const TCOORD4 crdq = cgw_lj.image[tmp_snd_img_idx];
#    else
          const TCOORD4 crdq = __ldg(&cgw_lj.image[tmp_snd_img_idx]);
#    endif
          tmp_snd_xcrd = crdq.x;
          tmp_snd_ycrd = crdq.y;
          tmp_snd_zcrd = crdq.z;
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
          sending_ljidx = __double_as_longlong(crdq.w);
#      else
          sending_ljidx = __float_as_int(crdq.w);
#      endif
#    else
          sending_ljidx = crdq.w;
#    endif
        }
#  else // DUAL_GRIDS
        tmp_snd_topl_idx = __ldg(&cgw.nonimg_atom_idx[tmp_snd_img_idx]);
        tmp_snd_prof_idx = __ldg(&lemr.prof_idx[tmp_snd_topl_idx]);
#    ifdef TCOORD_IS_LONG
        const TCOORD4 crdq = cgw.image[tmp_snd_img_idx];
#    else
        const TCOORD4 crdq = __ldg(&cgw.image[tmp_snd_img_idx]);
#    endif
        tmp_snd_xcrd = crdq.x;
        tmp_snd_ycrd = crdq.y;
        tmp_snd_zcrd = crdq.z;
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
        const llint fused_param_idx = __double_as_longlong(crdq.w);
        sending_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & dp_charge_index_mask)]);
        sending_ljidx = (fused_param_idx >> dp_charge_index_bits);
#      else
        const int fused_param_idx = __float_as_int(crdq.w);
        sending_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & sp_charge_index_mask)]);
        sending_ljidx = (fused_param_idx >> sp_charge_index_bits);
#      endif
#    else
#      ifdef TCOORD_IS_LONG
        sending_q     = __ldca(&poly_nbk.q_params[(crdq.w & dp_charge_index_mask)]);
        sending_ljidx = (crdq.w >> dp_charge_index_bits);
#      else
        sending_q     = __ldca(&poly_nbk.q_params[(crdq.w & sp_charge_index_mask)]);
        sending_ljidx = (crdq.w >> sp_charge_index_bits);
#      endif
#    endif
#  endif
        // Once again, modify the Lennard-Jones (van-der Waals) parameter index with the correct
        // offset in the synthesis compilation of tables.  Modify the charge parameter by folding
        // in Coulomb's constant at this stage.
        const int sys_idx = system_idx[warp_idx];
        sending_ljidx = (sending_ljidx * __ldca(&poly_nbk.n_lj_types[sys_idx])) +
                        __ldca(&poly_nbk.ljabc_offsets[sys_idx]);
#  ifndef TCOORD_IS_REAL
#    ifdef DUAL_GRIDS
        if (warp_on_qq[warp_idx]) {
          tmp_snd_xcrd *= cgw_qq.lpos_inv_scale;
          tmp_snd_ycrd *= cgw_qq.lpos_inv_scale;
          tmp_snd_zcrd *= cgw_qq.lpos_inv_scale;
        }
        else {
          tmp_snd_xcrd *= cgw_lj.lpos_inv_scale;
          tmp_snd_ycrd *= cgw_lj.lpos_inv_scale;
          tmp_snd_zcrd *= cgw_lj.lpos_inv_scale;
        }
#    else
        tmp_snd_xcrd *= cgw.lpos_inv_scale;
        tmp_snd_ycrd *= cgw.lpos_inv_scale;
        tmp_snd_zcrd *= cgw.lpos_inv_scale;
#    endif
#  endif

      }
      
      // Log the coordinates and particle indices of sending atoms in a warp-synchronous manner,
      // as these __shared__memory accesses are well coalesced but branching can be avoided.
      sending_xcrd[threadIdx.x] = tmp_snd_xcrd;
      sending_ycrd[threadIdx.x] = tmp_snd_ycrd;
      sending_zcrd[threadIdx.x] = tmp_snd_zcrd;
      sending_img_idx[threadIdx.x] = tmp_snd_img_idx;
      sending_topl_idx[threadIdx.x] = tmp_snd_topl_idx;
      sending_prof_idx[threadIdx.x] = tmp_snd_prof_idx;
      
      // The sending atoms' plan layout must again be established for all threads in the warp,
      // requiring a thread synchronization.
      SYNCWARP;

      // Loop over all atoms in the center cell, avoiding double-counting by using the special
      // "self interactions" tiles when warranted.
      for (uint j = center_base_pos[warp_idx]; j <= i; j += warp_size_uint) {
        const int jbatch_size = center_max_pos[warp_idx] - j;
        int tile_depth, plan_idx;
        if (jbatch_size > half_warp_size_int) {
          tile_depth = self_tile_depth[warp_idx];
          plan_idx = self_plan_idx[warp_idx];
        }
        else if (jbatch_size > quarter_warp_size_int) {
          tile_depth = (self_tile_depth[warp_idx] >> 1);
          plan_idx = self_plan_idx[warp_idx] + 1;
        }
        else {
          tile_depth = (self_tile_depth[warp_idx] >> 2);
          plan_idx = self_plan_idx[warp_idx] + 2;
        }

        // Consider self-interactions if appropriate.
        if (j == i) {
          tile_depth >>= 1;
        }

        // Read atomic coordinates and properties for the receiving atoms.
        TCOORD recv_xcrd, recv_ycrd, recv_zcrd;
        TCALC recv_q;
        int recv_ljidx, recv_topl_idx;
        uint img_idx = j + __ldca(&tlpn.self_assign[(plan_idx * warp_size_int) + lane_idx]);
        if (img_idx >= center_max_pos[warp_idx]) {
#  ifdef DUAL_GRIDS
          const TCALC fake_pos = (warp_on_qq[warp_idx]) ?
                                 (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.elec_cut) :
                                 (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  else
          const TCALC fake_pos = (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  endif
          recv_xcrd = -fake_pos;
          recv_ycrd = -fake_pos;
          recv_zcrd = -fake_pos;
          recv_ljidx = 0;
          recv_q = (TCALC)(0.0);
          recv_topl_idx = 0;
          recving_img_idx[threadIdx.x] = 0xffffffff;
        }
        else {
          recving_img_idx[threadIdx.x] = img_idx;
#  ifdef DUAL_GRIDS
          if (warp_on_qq[warp_idx]) {
#    ifdef TCOORD_IS_LONG
            const TCOORD4 crdq = cgw_qq.image[img_idx];
#    else
            const TCOORD4 crdq = __ldg(&cgw_qq.image[img_idx]);
#    endif
            recv_topl_idx = __ldg(&cgw_qq.nonimg_atom_idx[img_idx]);
            recv_xcrd = crdq.x;
            recv_ycrd = crdq.y;
            recv_zcrd = crdq.z;
#    ifdef TCOORD_IS_REAL
            recv_q = crdq.w;
#    else
#      ifdef TCOORD_IS_LONG
            recv_q = __longlong_as_double(crdq.w);
#      else
            recv_q = __int_as_float(crdq.w);
#      endif
#    endif
          }
          else {
#    ifdef TCOORD_IS_LONG
            const TCOORD4 crdq = cgw_lj.image[img_idx];
#    else
            const TCOORD4 crdq = __ldg(&cgw_lj.image[img_idx]);
#    endif
            recv_topl_idx = __ldg(&cgw_lj.nonimg_atom_idx[img_idx]);
            recv_xcrd = crdq.x;
            recv_ycrd = crdq.y;
            recv_zcrd = crdq.z;
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
            recv_ljidx = __double_as_longlong(crdq.w);
#      else
            recv_ljidx = __float_as_int(crdq.w);
#      endif
#    else
            recv_ljidx = crdq.w;
#    endif
          }
#  else // DUAL_GRIDS
#    ifdef TCOORD_IS_LONG
          const TCOORD4 crdq = cgw.image[img_idx];
#    else
          const TCOORD4 crdq = __ldg(&cgw.image[img_idx]);
#    endif
          recv_topl_idx = __ldg(&cgw.nonimg_atom_idx[img_idx]);
          recv_xcrd = crdq.x;
          recv_ycrd = crdq.y;
          recv_zcrd = crdq.z;
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
          const llint fused_param_idx = __double_as_longlong(crdq.w);
          recv_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & dp_charge_index_mask)]);
          recv_ljidx = (fused_param_idx >> dp_charge_index_bits);
#      else
          const int fused_param_idx = __float_as_int(crdq.w);
          recv_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & sp_charge_index_mask)]);
          recv_ljidx = (fused_param_idx >> sp_charge_index_bits);
#      endif
#    else
#      ifdef TCOORD_IS_LONG
          recv_q     = __ldca(&poly_nbk.q_params[(crdq.w & dp_charge_index_mask)]);
          recv_ljidx = (crdq.w >> dp_charge_index_bits);
#      else
          recv_q     = __ldca(&poly_nbk.q_params[(crdq.w & sp_charge_index_mask)]);
          recv_ljidx = (crdq.w >> sp_charge_index_bits);
#      endif
#    endif
#  endif // DUAL_GRIDS
        }
                  
        // Evaluate the tile
        TCALC send_fx = (TCALC)(0.0);
        TCALC send_fy = (TCALC)(0.0);
        TCALC send_fz = (TCALC)(0.0);
        TCALC recv_fx = (TCALC)(0.0);
        TCALC recv_fy = (TCALC)(0.0);
        TCALC recv_fz = (TCALC)(0.0);
#  ifdef DUAL_GRIDS
        if (warp_on_qq[warp_idx]) {
          for (int k = 0; k < tile_depth; k++) {
            const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
            const uint lkp_advance = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                           recv_topl_idx, excl_mask,
                                                           lemr.aux_masks) ?
                                     nrg_tab.excl_offset : 0;
                
            // Compute the interaction between the atoms.
            const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
            const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
            const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x]; 
            const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
            const TCALC qq = recv_q * sending_q * (TCALC)(r2 < ctrl.elec_cut_sq);
#    ifdef TCALC_IS_SINGLE
            const uint spl_idx = (__float_as_uint(r2) >> nrg_tab.index_shift_bits) + lkp_advance;
#    else
            const uint spl_idx = (uint)(__double_as_longlong(r2) >> nrg_tab.index_shift_bits) +
                                 lkp_advance;
#    endif
            const TCALC4 coef = nrg_tab.force[spl_idx];
            const TCALC invr2 = (TCALC)(1.0) / r2;
            TCALC fmag = qq * ((r2 * coef.x) + coef.y +
                               (((invr2 * coef.w) + coef.z) * invr2));
            if (i == j && k == 0) {
              const TCALC attn = __ldca(&tlpn.scalings[(plan_idx * warp_size_int) + lane_idx]);
              fmag *= attn;
            }
            send_fx += fmag * dx;
            send_fy += fmag * dy;
            send_fz += fmag * dz;
            recv_fx -= fmag * dx;
            recv_fy -= fmag * dy;
            recv_fz -= fmag * dz;
            const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
            recv_xcrd = SHFL(recv_xcrd, next_thread);
            recv_ycrd = SHFL(recv_ycrd, next_thread);
            recv_zcrd = SHFL(recv_zcrd, next_thread);
            recv_fx = SHFL(recv_fx, next_thread);
            recv_fy = SHFL(recv_fy, next_thread);
            recv_fz = SHFL(recv_fz, next_thread);
            recv_q = SHFL(recv_q, next_thread);
            recv_topl_idx = SHFL(recv_topl_idx, next_thread);
          }
          storeRecvForces(center_max_pos[warp_idx] - j, lane_idx, self_plan_idx[warp_idx],
                          tlpn.self_assign, recv_fx, recv_fy, recv_fz, cgw_qq.frc_scale,
                          recving_img_idx[threadIdx.x], cgw_qq.xfrc, cgw_qq.yfrc, cgw_qq.zfrc,
                          cgw_qq.xfrc_ovrf, cgw_qq.yfrc_ovrf, cgw_qq.zfrc_ovrf);
        }
        else {
          for (int k = 0; k < tile_depth; k++) {
            const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
            const bool is_excl = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                       recv_topl_idx, excl_mask, lemr.aux_masks);
            const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
            const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
            const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x]; 
            const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
            const TCALC valid = (r2 < ctrl.vdw_cut_sq && (! is_excl));
            const int klj_idx = sending_ljidx + recv_ljidx;
            const TCALC lja = __ldca(&poly_nbk.lja_coeff[klj_idx]);
            const TCALC ljb = __ldca(&poly_nbk.ljb_coeff[klj_idx]);
            const TCALC invr2 = (TCALC)(1.0) / r2;
            const TCALC invr4 = invr2 * invr2;
            TCALC fmag = (((TCALC)(6.0) * ljb) - ((TCALC)(12.0) * lja * invr4 * invr2)) *
                         invr4 * invr4 * valid;
            if (i == j && k == 0) {
              const TCALC attn = __ldca(&tlpn.scalings[(plan_idx * warp_size_int) + lane_idx]);
              fmag *= attn;
            }
            send_fx += fmag * dx;
            send_fy += fmag * dy;
            send_fz += fmag * dz;
            recv_fx -= fmag * dx;
            recv_fy -= fmag * dy;
            recv_fz -= fmag * dz;
            const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
            recv_xcrd = SHFL(recv_xcrd, next_thread);
            recv_ycrd = SHFL(recv_ycrd, next_thread);
            recv_zcrd = SHFL(recv_zcrd, next_thread);
            recv_fx = SHFL(recv_fx, next_thread);
            recv_fy = SHFL(recv_fy, next_thread);
            recv_fz = SHFL(recv_fz, next_thread);
            recv_ljidx = SHFL(recv_ljidx, next_thread);
            recv_topl_idx = SHFL(recv_topl_idx, next_thread);
          }
          storeRecvForces(center_max_pos[warp_idx] - j, lane_idx, self_plan_idx[warp_idx],
                          tlpn.self_assign, recv_fx, recv_fy, recv_fz, cgw_lj.frc_scale,
                          recving_img_idx[threadIdx.x], cgw_lj.xfrc, cgw_lj.yfrc, cgw_lj.zfrc,
                          cgw_lj.xfrc_ovrf, cgw_lj.yfrc_ovrf, cgw_lj.zfrc_ovrf);
        }
#  else // DUAL_GRIDS
        for (int k = 0; k < tile_depth; k++) {
          const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
          const uint lkp_advance = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                         recv_topl_idx, excl_mask,
                                                         lemr.aux_masks) ?
                                   nrg_tab.excl_offset : 0;
          const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
          const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
          const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x]; 
          const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
          const TCALC in_range = (r2 < ctrl.vdw_cut_sq);
          const TCALC qq = recv_q * sending_q * in_range;
#    ifdef TCALC_IS_SINGLE
          const uint spl_idx = (__float_as_uint(r2) >> nrg_tab.index_shift_bits) + lkp_advance;
#    else
          const uint spl_idx = (uint)(__double_as_longlong(r2) >> nrg_tab.index_shift_bits) +
                               lkp_advance;
#    endif
          const TCALC4 coef = nrg_tab.force[spl_idx];
          const TCALC invr2 = (TCALC)(1.0) / r2;
          TCALC fmag = qq * ((r2 * coef.x) + coef.y + (((invr2 * coef.w) + coef.z) * invr2));
          const int klj_idx = sending_ljidx + recv_ljidx;
          const TCALC lja = __ldca(&poly_nbk.lja_coeff[klj_idx]);
          const TCALC ljb = __ldca(&poly_nbk.ljb_coeff[klj_idx]);
          const TCALC invr4 = invr2 * invr2;
          fmag += (((TCALC)(6.0) * ljb) - ((TCALC)(12.0) * lja * invr4 * invr2)) *
                  invr4 * invr4 * in_range * (TCALC)(lkp_advance == 0U);
          if (i == j && k == 0) {
            const TCALC attn = __ldca(&tlpn.scalings[(plan_idx * warp_size_int) + lane_idx]);
            fmag *= attn;
          }

          // CHECK
#if 0
          if (fabsf(fmag) > (TCALC)(1.0e-6)) {
            const char excl_char = (lkp_advance > 0) ? 'E' : ' ';
            printf("T-Tw %4d - %4d on %2d (%2d) :: [ %9.5f %9.5f %12.4f %12.4f ] %c at %9.4f "
                   "%9.4f %9.4f [ %6u ] [ %9.4f %9.4f %9.4f  %9.4f %9.4f %9.4f ] -> %9.4f\n",
                   sending_topl_idx[threadIdx.x], recv_topl_idx, lane_idx, k, sending_q, recv_q,
                   lja, ljb, excl_char, dx, dy, dz, spl_idx, sending_xcrd[threadIdx.x],
                   sending_ycrd[threadIdx.x], sending_zcrd[threadIdx.x], recv_xcrd, recv_ycrd,
                   recv_zcrd, fmag);
          }
#endif
          // END CHECK
          
          send_fx += fmag * dx;
          send_fy += fmag * dy;
          send_fz += fmag * dz;
          recv_fx -= fmag * dx;
          recv_fy -= fmag * dy;
          recv_fz -= fmag * dz;
          const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
          recv_xcrd = SHFL(recv_xcrd, next_thread);
          recv_ycrd = SHFL(recv_ycrd, next_thread);
          recv_zcrd = SHFL(recv_zcrd, next_thread);
          recv_fx = SHFL(recv_fx, next_thread);
          recv_fy = SHFL(recv_fy, next_thread);
          recv_fz = SHFL(recv_fz, next_thread);
          recv_q = SHFL(recv_q, next_thread);
          recv_ljidx = SHFL(recv_ljidx, next_thread);
          recv_topl_idx = SHFL(recv_topl_idx, next_thread);
        }
        storeRecvForces(center_max_pos[warp_idx] - j, lane_idx, self_plan_idx[warp_idx],
                        tlpn.self_assign, recv_fx, recv_fy, recv_fz, cgw.frc_scale,
                        recving_img_idx[threadIdx.x], cgw.xfrc, cgw.yfrc, cgw.zfrc,
                        cgw.xfrc_ovrf, cgw.yfrc_ovrf, cgw.zfrc_ovrf);
#  endif // DUAL_GRIDS
          
        // Hold on reducing the forces on sending atoms until all receiving atoms have been
        // processed.  Commit results to local accumulators.
#  ifdef DUAL_GRIDS
        const TCALC frc_scale = cgw_qq.frc_scale;
#  else
        const TCALC frc_scale = cgw.frc_scale;
#  endif
#  ifdef LARGE_CHIP_CACHE
        cacheSendForces(send_fx, frc_scale, sending_xfrc, sending_xfrc_ovrf);
        cacheSendForces(send_fy, frc_scale, sending_yfrc, sending_yfrc_ovrf);
        cacheSendForces(send_fz, frc_scale, sending_zfrc, sending_zfrc_ovrf);
#  else
        cacheSendForces(send_fx, frc_scale, sending_xfrc, tlpn.xfrc_ovrf);
        cacheSendForces(send_fy, frc_scale, sending_yfrc, tlpn.yfrc_ovrf);
        cacheSendForces(send_fz, frc_scale, sending_zfrc, tlpn.zfrc_ovrf);
#  endif
      }

      // Process atoms in the lower tower using the same sending atom layout.
      for (uint j = 0; j < tower_prefix_sum[(warp_idx * 6) + 2]; j += warp_size_uint) {
        
        // Again, temporarily take critical values from the tower prefix sum into registers.
        const int trpfx_1 = tower_prefix_sum[(warp_idx * 6) + 1];
        const int trpfx_2 = tower_prefix_sum[(warp_idx * 6) + 2];

        // Find the appropriate plan index.  The final plan will be based off of the "self"
        // interactions sending atom plan, but the plan number thus derived will be used to get
        // a standard indexing layout for the receiving atoms.  They will never be one and the
        // same with the sending atoms.
        const int jbatch_size = trpfx_2 - j;
        int tile_depth, plan_idx;
        if (jbatch_size > half_warp_size_int) {
          tile_depth = self_tile_depth[warp_idx];
          plan_idx = self_plan_idx[warp_idx];
        }
        else if (jbatch_size > quarter_warp_size_int) {
          tile_depth = (self_tile_depth[warp_idx] >> 1);
          plan_idx = self_plan_idx[warp_idx] + 1;
        }
        else {
          tile_depth = (self_tile_depth[warp_idx] >> 2);
          plan_idx = self_plan_idx[warp_idx] + 2;
        }

        // Read atomic coordinates and properties for the receiving atoms.
        TCOORD recv_xcrd, recv_ycrd, recv_zcrd;
        TCALC recv_q;
        int recv_ljidx, recv_topl_idx;
        uint jatom_seek = j + __ldca(&tlpn.read_assign[(plan_idx * warp_size_int) + lane_idx]);
        if (jatom_seek >= trpfx_2) {
#  ifdef DUAL_GRIDS
          const TCALC fake_pos = (warp_on_qq[warp_idx]) ?
                                 (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.elec_cut) :
                                 (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  else
          const TCALC fake_pos = (TCALC)(lane_idx) + ((TCALC)(2.0) * ctrl.vdw_cut);
#  endif
          recv_xcrd = -fake_pos;
          recv_ycrd = -fake_pos;
          recv_zcrd = -fake_pos;
          recv_ljidx = 0;
          recv_q = (TCALC)(0.0);
          recv_topl_idx = 0;
          recving_img_idx[threadIdx.x] = 0xffffffff;
        }
        else {
          uint img_idx;
          TCOORD zc_moves;
          if (jatom_seek >= trpfx_1) {
            zc_moves = (TCOORD)(-1.0);
            img_idx = tower_cg_offsets[(warp_idx * 5) + 1] + jatom_seek - trpfx_1;
          }
          else {
            zc_moves = (TCOORD)(-2.0);
            img_idx = tower_cg_offsets[(warp_idx * 5) + 1] + jatom_seek;
          }
          const size_t cinvu_idx = system_idx[warp_idx] * warp_size_int;
#  ifdef DUAL_GRIDS
          if (warp_on_qq[warp_idx]) {
#    ifdef TCOORD_IS_LONG
            const TCOORD4 crdq = cgw_qq.image[img_idx];
#    else
            const TCOORD4 crdq = __ldg(&cgw_qq.image[img_idx]);
#    endif
            recv_topl_idx = __ldg(&cgw_qq.nonimg_atom_idx[img_idx]);
            recv_xcrd = crdq.x + (zc_moves * cgw_qq.system_cell_invu[cinvu_idx + 6]);
            recv_ycrd = crdq.y + (zc_moves * cgw_qq.system_cell_invu[cinvu_idx + 7]);
            recv_zcrd = crdq.z + (zc_moves * cgw_qq.system_cell_invu[cinvu_idx + 8]);
#    ifdef TCOORD_IS_REAL
            recv_q = crdq.w;
#    else
#      ifdef TCOORD_IS_LONG
            recv_q = __longlong_as_double(crdq.w);
#      else
            recv_q = __int_as_float(crdq.w);
#      endif
#    endif
          }
          else {
#    ifdef TCOORD_IS_LONG
            const TCOORD4 crdq = cgw_lj.image[img_idx];
#    else
            const TCOORD4 crdq = __ldg(&cgw_lj.image[img_idx]);
#    endif
            recv_topl_idx = __ldg(&cgw_lj.nonimg_atom_idx[img_idx]);
            recv_xcrd = crdq.x + (zc_moves * cgw_lj.system_cell_invu[cinvu_idx + 6]);
            recv_ycrd = crdq.y + (zc_moves * cgw_lj.system_cell_invu[cinvu_idx + 7]);
            recv_zcrd = crdq.z + (zc_moves * cgw_lj.system_cell_invu[cinvu_idx + 8]);
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
            recv_ljidx = __double_as_longlong(crdq.w);
#      else
            recv_ljidx = __float_as_int(crdq.w);
#      endif
#    else
            recv_ljidx = crdq.w;
#    endif
          }
#  else // DUAL_GRIDS
#    ifdef TCOORD_IS_LONG
          const TCOORD4 crdq = cgw.image[img_idx];
#    else
          const TCOORD4 crdq = __ldg(&cgw.image[img_idx]);
#    endif
          recv_topl_idx = __ldg(&cgw.nonimg_atom_idx[img_idx]);
          recv_xcrd = crdq.x + (zc_moves * cgw.system_cell_invu[cinvu_idx + 6]);
          recv_ycrd = crdq.y + (zc_moves * cgw.system_cell_invu[cinvu_idx + 7]);
          recv_zcrd = crdq.z + (zc_moves * cgw.system_cell_invu[cinvu_idx + 8]);
#    ifdef TCOORD_IS_REAL
#      ifdef TCOORD_IS_LONG
          const llint fused_param_idx = __double_as_longlong(crdq.w);
          recv_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & dp_charge_index_mask)]);
          recv_ljidx = (fused_param_idx >> dp_charge_index_bits);
#      else
          const int fused_param_idx = __float_as_int(crdq.w);
          recv_q     = __ldca(&poly_nbk.q_params[(fused_param_idx & sp_charge_index_mask)]);
          recv_ljidx = (fused_param_idx >> sp_charge_index_bits);
#      endif
#    else
#      ifdef TCOORD_IS_LONG
          recv_q     = __ldca(&poly_nbk.q_params[(crdq.w & dp_charge_index_mask)]);
          recv_ljidx = (crdq.w >> dp_charge_index_bits);
#      else
          recv_q     = __ldca(&poly_nbk.q_params[(crdq.w & sp_charge_index_mask)]);
          recv_ljidx = (crdq.w >> sp_charge_index_bits);
#      endif
#    endif
#  endif // DUAL_GRIDS
#  ifndef TCOORD_IS_REAL
#    ifdef DUAL_GRIDS
          if (warp_on_qq[warp_idx]) {
            recv_xcrd *= cgw_qq.lpos_inv_scale;
            recv_ycrd *= cgw_qq.lpos_inv_scale;
            recv_zcrd *= cgw_qq.lpos_inv_scale;
          }
          else {
            recv_xcrd *= cgw_lj.lpos_inv_scale;
            recv_ycrd *= cgw_lj.lpos_inv_scale;
            recv_zcrd *= cgw_lj.lpos_inv_scale;
          }
#    else
          recv_xcrd *= cgw.lpos_inv_scale;
          recv_ycrd *= cgw.lpos_inv_scale;
          recv_zcrd *= cgw.lpos_inv_scale;
#    endif
#  endif // TCOORD_IS_REAL
        }
        
        // Iterate the tile as before.  There is no need to scale the forces or pairwise energy
        // contributions in this case.
        TCALC send_fx = (TCALC)(0.0);
        TCALC send_fy = (TCALC)(0.0);
        TCALC send_fz = (TCALC)(0.0);
        TCALC recv_fx = (TCALC)(0.0);
        TCALC recv_fy = (TCALC)(0.0);
        TCALC recv_fz = (TCALC)(0.0);
#  ifdef DUAL_GRIDS
        if (warp_on_qq[warp_idx]) {
          for (int k = 0; k < tile_depth; k++) {
            const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
            const uint lkp_advance = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                           recv_topl_idx, excl_mask,
                                                           lemr.aux_masks) ?
                                     nrg_tab.excl_offset : 0;
            const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
            const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
            const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x];
            const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
            const TCALC qq = recv_q * sending_q * (TCALC)(r2 < ctrl.elec_cut_sq);
#    ifdef TCALC_IS_SINGLE
            const uint spl_idx = (__float_as_uint(r2) >> nrg_tab.index_shift_bits) + lkp_advance;
#    else
            const uint spl_idx = (uint)(__double_as_longlong(r2) >> nrg_tab.index_shift_bits) +
                                 lkp_advance;
#    endif
            const TCALC4 coef = nrg_tab.force[spl_idx];
            const TCALC invr2 = (TCALC)(1.0) / r2;
            const TCALC fmag = qq * ((r2 * coef.x) + coef.y +
                                     (((invr2 * coef.w) + coef.z) * invr2));
            send_fx += fmag * dx;
            send_fy += fmag * dy;
            send_fz += fmag * dz;
            recv_fx -= fmag * dx;
            recv_fy -= fmag * dy;
            recv_fz -= fmag * dz;
            const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
            recv_xcrd = SHFL(recv_xcrd, next_thread);
            recv_ycrd = SHFL(recv_ycrd, next_thread);
            recv_zcrd = SHFL(recv_zcrd, next_thread);
            recv_fx = SHFL(recv_fx, next_thread);
            recv_fy = SHFL(recv_fy, next_thread);
            recv_fz = SHFL(recv_fz, next_thread);
            recv_q = SHFL(recv_q, next_thread);
            recv_topl_idx = SHFL(recv_topl_idx, next_thread);
          }
          storeRecvForces<TCALC, TACC>(tower_prefix_sum[(warp_idx * 6) + 2] - j, lane_idx,
                                       self_plan_idx[warp_idx], tlpn.reduce_prep, recv_fx,
                                       recv_fy, recv_fz, cgw_qq.frc_scale,
                                       recving_img_idx[threadIdx.x], cgw_qq.xfrc, cgw_qq.yfrc,
                                       cgw_qq.zfrc, cgw_qq.xfrc_ovrf, cgw_qq.yfrc_ovrf,
                                       cgw_qq.zfrc_ovrf);
        }
        else {
          for (int k = 0; k < tile_depth; k++) {
            const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
            const bool is_excl = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                       recv_topl_idx, excl_mask, lemr.aux_masks);
            const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
            const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
            const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x];
            const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
            const TCALC valid = (r2 < ctrl.vdw_cut_sq && (! is_excl));
            const int klj_idx = sending_ljidx + recv_ljidx;
            const TCALC lja = __ldca(&poly_nbk.lja_coeff[klj_idx]);
            const TCALC ljb = __ldca(&poly_nbk.ljb_coeff[klj_idx]);
            const TCALC invr2 = (TCALC)(1.0) / r2;
            const TCALC invr4 = invr2 * invr2;
            const TCALC fmag = (((TCALC)(6.0) * ljb) - ((TCALC)(12.0) * lja * invr4 * invr2)) *
                               invr4 * invr4 * valid;
            send_fx += fmag * dx;
            send_fy += fmag * dy;
            send_fz += fmag * dz;
            recv_fx -= fmag * dx;
            recv_fy -= fmag * dy;
            recv_fz -= fmag * dz;
            const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
            recv_xcrd = SHFL(recv_xcrd, next_thread);
            recv_ycrd = SHFL(recv_ycrd, next_thread);
            recv_zcrd = SHFL(recv_zcrd, next_thread);
            recv_fx = SHFL(recv_fx, next_thread);
            recv_fy = SHFL(recv_fy, next_thread);
            recv_fz = SHFL(recv_fz, next_thread);
            recv_ljidx = SHFL(recv_ljidx, next_thread);
            recv_topl_idx = SHFL(recv_topl_idx, next_thread);
          }
          storeRecvForces<TCALC, TACC>(tower_prefix_sum[(warp_idx * 6) + 2] - j, lane_idx,
                                       self_plan_idx[warp_idx], tlpn.reduce_prep, recv_fx,
                                       recv_fy, recv_fz, cgw_lj.frc_scale,
                                       recving_img_idx[threadIdx.x], cgw_lj.xfrc, cgw_lj.yfrc,
                                       cgw_lj.zfrc, cgw_lj.xfrc_ovrf, cgw_lj.yfrc_ovrf,
                                       cgw_lj.zfrc_ovrf);
        }
#  else // DUAL_GRIDS
        for (int k = 0; k < tile_depth; k++) {
          const ullint excl_mask = __ldca(&lemr.profiles[sending_prof_idx[threadIdx.x]]);
          const uint lkp_advance = devcEvaluateLocalMask(sending_topl_idx[threadIdx.x],
                                                         recv_topl_idx, excl_mask,
                                                         lemr.aux_masks) ?
                                   nrg_tab.excl_offset : 0;
          const TCOORD dx = recv_xcrd - sending_xcrd[threadIdx.x];
          const TCOORD dy = recv_ycrd - sending_ycrd[threadIdx.x];
          const TCOORD dz = recv_zcrd - sending_zcrd[threadIdx.x];
          const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
          const TCALC in_range = (r2 < ctrl.vdw_cut_sq);
          const TCALC qq = recv_q * sending_q * in_range;
#    ifdef TCALC_IS_SINGLE
          const uint spl_idx = (__float_as_uint(r2) >> nrg_tab.index_shift_bits) + lkp_advance;
#    else
          const uint spl_idx = (uint)(__double_as_longlong(r2) >> nrg_tab.index_shift_bits) +
                               lkp_advance;
#    endif
          const TCALC4 coef = nrg_tab.force[spl_idx];
          const TCALC invr2 = (TCALC)(1.0) / r2;
          TCALC fmag = qq * ((r2 * coef.x) + coef.y + (((invr2 * coef.w) + coef.z) * invr2));
          const int klj_idx = sending_ljidx + recv_ljidx;
          const TCALC lja = __ldca(&poly_nbk.lja_coeff[klj_idx]);
          const TCALC ljb = __ldca(&poly_nbk.ljb_coeff[klj_idx]);
          const TCALC invr4 = invr2 * invr2;
          fmag += (((TCALC)(6.0) * ljb) - ((TCALC)(12.0) * lja * invr4 * invr2)) *
                  invr4 * invr4 * in_range * (TCALC)(lkp_advance == 0U);

          // CHECK
#if 0
          if (fabsf(fmag) > (TCALC)(1.0e-6)) {
            const char excl_char = (lkp_advance > 0) ? 'E' : ' ';
            printf("T-Tw %4d - %4d on %2d (%2d) :: [ %9.5f %9.5f %12.4f %12.4f ] %c at %9.4f "
                   "%9.4f %9.4f [ %6u ] [ %9.4f %9.4f %9.4f  %9.4f %9.4f %9.4f ] -> %9.4f\n",
                   sending_topl_idx[threadIdx.x], recv_topl_idx, lane_idx, k, sending_q, recv_q,
                   lja, ljb, excl_char, dx, dy, dz, spl_idx, sending_xcrd[threadIdx.x],
                   sending_ycrd[threadIdx.x], sending_zcrd[threadIdx.x], recv_xcrd, recv_ycrd,
                   recv_zcrd, fmag);
          }
#endif
          // END CHECK
          
          send_fx += fmag * dx;
          send_fy += fmag * dy;
          send_fz += fmag * dz;
          recv_fx -= fmag * dx;
          recv_fy -= fmag * dy;
          recv_fz -= fmag * dz;
          const int next_thread = ((lane_idx + 1) & warp_bits_mask_int);
          recv_xcrd = SHFL(recv_xcrd, next_thread);
          recv_ycrd = SHFL(recv_ycrd, next_thread);
          recv_zcrd = SHFL(recv_zcrd, next_thread);
          recv_fx = SHFL(recv_fx, next_thread);
          recv_fy = SHFL(recv_fy, next_thread);
          recv_fz = SHFL(recv_fz, next_thread);
          recv_q = SHFL(recv_q, next_thread);
          recv_ljidx = SHFL(recv_ljidx, next_thread);
          recv_topl_idx = SHFL(recv_topl_idx, next_thread);
        }
        storeRecvForces<TCALC, TACC>(tower_prefix_sum[(warp_idx * 6) + 2] - j, lane_idx,
                                     self_plan_idx[warp_idx], tlpn.reduce_prep, recv_fx, recv_fy,
                                     recv_fz, cgw.frc_scale, recving_img_idx[threadIdx.x],
                                     cgw.xfrc, cgw.yfrc, cgw.zfrc, cgw.xfrc_ovrf, cgw.yfrc_ovrf,
                                     cgw.zfrc_ovrf);
#  endif // DUAL_GRIDS
      }

      // Reduce the accumulated forces on sending atoms, if required.  Commit the results to
      // global arrays by atomic operations.
      const size_t thr_idx_zu = threadIdx.x;
#  ifdef LARGE_CHIP_CACHE
      const int tmp_x_ovrf = sending_xfrc_ovrf[thr_idx_zu];
      const int tmp_y_ovrf = sending_yfrc_ovrf[thr_idx_zu];
      const int tmp_z_ovrf = sending_zfrc_ovrf[thr_idx_zu];
#  else
      const size_t gbl_cache_idx = (blockIdx.x * blockDim.x) + threadIdx.x;
      const int tmp_x_ovrf = tlpn.xfrc_ovrf[gbl_cache_idx];
      const int tmp_y_ovrf = tlpn.yfrc_ovrf[gbl_cache_idx];
      const int tmp_z_ovrf = tlpn.zfrc_ovrf[gbl_cache_idx];
#  endif
#  ifdef DUAL_GRIDS
      if (warp_on_qq[warp_idx]) {
        storeSendForces<TCALC>(lane_idx, base_plan_idx[warp_idx], sending_xfrc[thr_idx_zu],
                               sending_yfrc[thr_idx_zu], sending_zfrc[thr_idx_zu], tmp_x_ovrf,
                               tmp_y_ovrf, tmp_z_ovrf, sending_img_idx[thr_idx_zu], cgw_qq.xfrc,
                               cgw_qq.yfrc, cgw_qq.zfrc, cgw_qq.xfrc_ovrf, cgw_qq.yfrc_ovrf,
                               cgw_qq.zfrc_ovrf);
      }
      else {
        storeSendForces<TCALC>(lane_idx, base_plan_idx[warp_idx], sending_xfrc[thr_idx_zu],
                               sending_yfrc[thr_idx_zu], sending_zfrc[thr_idx_zu], tmp_x_ovrf,
                               tmp_y_ovrf, tmp_z_ovrf, sending_img_idx[thr_idx_zu], cgw_lj.xfrc,
                               cgw_lj.yfrc, cgw_lj.zfrc, cgw_lj.xfrc_ovrf, cgw_lj.yfrc_ovrf,
                               cgw_lj.zfrc_ovrf);
      }
#  else // DUAL_GRIDS
      storeSendForces<TCALC>(lane_idx, base_plan_idx[warp_idx], sending_xfrc[thr_idx_zu],
                             sending_yfrc[thr_idx_zu], sending_zfrc[thr_idx_zu], tmp_x_ovrf,
                             tmp_y_ovrf, tmp_z_ovrf, sending_img_idx[thr_idx_zu], cgw.xfrc,
                             cgw.yfrc, cgw.zfrc, cgw.xfrc_ovrf, cgw.yfrc_ovrf, cgw.zfrc_ovrf);
#  endif
    }

    // Increment the work unit counter.
    SYNCWARP;
    if (lane_idx == 0) {
      warp_cell_counters[warp_idx] += PMENB_WARPS_PER_BLOCK * gridDim.x;
    }
    SYNCWARP;
#ifdef DUAL_GRIDS
  }
#else
  }
#endif
}

// Clear the definition of the thread count per block
#ifdef STORMM_USE_CUDA
#  undef PMENB_THREAD_COUNT
#endif
