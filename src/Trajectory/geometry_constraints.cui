// -*-c++-*-
#include "copyright.h"

// Define the block's atom capacity, if this code is not included in another block where it is
// already defined.  See Structure/valence_potential.cui for more details.
#ifdef CONSTRAINT_STANDALONE
#  define EXCL_GMEM_OFFSET  (blockIdx.x * gmem_r.max_atoms)
#  ifdef TCALC_IS_SINGLE
#    if INTEG_KERNEL_THREAD_COUNT == 64
#      define VALENCE_ATOM_CAPACITY eighth_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 128
#      define VALENCE_ATOM_CAPACITY quarter_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 256
#      define VALENCE_ATOM_CAPACITY half_valence_work_unit_atoms
#    else
#      define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#    endif
#  else
#    define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#  endif
#endif

/// \brief The velocity update kernel will handle import of critical information (atom positions as
///        were used to compute forces).  Both the standalone kernel and includable code will then
///        process the development particle velocities to ensure that they are orthogonal to the
///        displacement along the constrained bonds, to within the tolerance.  The kernel will
///        finish by updating velocities according to each work unit's responsibilities.
#ifdef CONSTRAINT_STANDALONE
__global__ void __launch_bounds__(INTEG_KERNEL_THREAD_COUNT, INTEG_BLOCK_MULTIPLICITY)
KERNEL_NAME(const SyValenceKit<TCALC> poly_vk, MMControlKit<TCALC> ctrl,
            PsSynthesisWriter poly_psw, const SyAtomUpdateKit<TCALC, TCALC2, TCALC4> poly_auk,
            const ThermostatWriter<TCALC> tstw, CacheResourceKit<TCALC> gmem_r) {

  // Arrays named sh_xfrc and the like will hold the velocities to facilitate incorporation of the
  // core code into the valence kernels.
#  ifdef SPLIT_FORCE_ACCUMULATION
#    ifdef TCALC_IS_SINGLE
  __shared__ int sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_zfrc[VALENCE_ATOM_CAPACITY];
#    else
  __shared__ llint sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zfrc[VALENCE_ATOM_CAPACITY];
#    endif
  __shared__ int sh_xfrc_overflow[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_yfrc_overflow[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_zfrc_overflow[VALENCE_ATOM_CAPACITY];
#  else
  // As with the valence kernels, not having a definition of split force accumulation implies that
  // 64-bit signed integer accumulation is active in single-precision calculation mode.
  __shared__ llint sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zfrc[VALENCE_ATOM_CAPACITY];
#  endif
  
#  ifdef TCALC_IS_SINGLE
  // In single-precision mode, the valence kernel (which can include the core of this code) will
  // allocate arrays for particle positions in its __shared__ partition of L1.  In double-precision
  // mode, use the cache thread block resources.  These positions will become the reference as
  // the force arrays (which are held in __shared__ no matter the valence kernel configuration) are
  // used to iterate and refine the developing positions.
  __shared__ llint sh_xcrd[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_ycrd[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zcrd[VALENCE_ATOM_CAPACITY];
#  else
  __shared__ int sh_atom_ljidx[VALENCE_ATOM_CAPACITY];
#  endif
  __shared__ int2 vwu_map[vwu_abstract_length];
  __shared__ int vwu_task_count[vwu_abstract_length];
  __shared__ int vwu_padded_task_count[vwu_abstract_length];
  __shared__ volatile int vwu_idx;
  __shared__ volatile TCALC rtoldt;
  
  // Each block takes its first valence work unit based on its block index.
  if (threadIdx.x == 0) {
    vwu_idx = blockIdx.x;
  }
  __syncthreads();
  while (vwu_idx < poly_vk.nvwu) {

    // The instruction set map is read and stored in __shared__ for convenience, and to ensure
    // that it never leaves cache.  The instructions themselves are "streamed," which for purposes
    // of this documentation means read from global, used once, and not cached.  Each block must be
    // at least vwu_abstract_length in size.
    if (threadIdx.x < vwu_abstract_length) {
      vwu_map[threadIdx.x] = __ldcv(&poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                                                           threadIdx.x]);
      vwu_task_count[threadIdx.x] = vwu_map[threadIdx.x].y - vwu_map[threadIdx.x].x;
      vwu_padded_task_count[threadIdx.x] = devcRoundUp(vwu_task_count[threadIdx.x], warp_size_int);
    }
    __syncthreads();
    
    // Import the updated atomic coordinates.  This employs all threads of the block, breaking up
    // each set of information at the warp level.  Once the position update has occurred, forces
    // are no longer relevant and the arrays can thus be repurposed.  If the core of this kernel is
    // called in the context of a valence work unit kernel, the naive positions update will have
    // been stored in the most proximate arrays for the particle positions to act as a reference
    // while the force arrays are used to iterate those positions.  Replicate that setup for the
    // standalone kernel as the coordinates are taken in from global memory.
    const int impt_llim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int impt_hlim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y;
    const int impt_count  = impt_hlim - impt_llim;
    const int impt_stride = devcRoundUp(impt_hlim - impt_llim, warp_size_int);
    int pos = threadIdx.x;
    while (pos < impt_stride) {
      if (pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
        const size_t write_idx = pos + EXCL_GMEM_OFFSET;
        const llint x_update = __ldcv(&poly_psw.xalt[global_read_idx]);
#  ifdef TCALC_IS_SINGLE
        sh_xcrd[pos]     = __ldcv(&poly_psw.xcrd[global_read_idx]);
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 x_tmp = longlongToInt63(x_update);
        sh_xfrc[pos]          = x_tmp.x;
        sh_xfrc_overflow[pos] = x_tmp.y;
#    else
        sh_xfrc[pos]          = __ldca(&poly_psw.xalt[global_read_idx]);
#    endif
        // Re-initialize the atomic Lennard-Jones parameter array to zero to prepare for marking
        // whether each particle was subjected to constraints.  This will become relevant for the
        // final particle velocity adjustment.  Once particles are moved, there is no need to know
        // their Lennard-Jones parameters anymore.
        __stwb(&gmem_r.lj_idx[write_idx], 0);
#  else
        const int x_update_ovrf = __ldcv(&poly_psw.xalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.xcrd[write_idx], __ldcv(&poly_psw.xcrd[global_read_idx]));
        __stwb(&gmem_r.xcrd_ovrf[write_idx], __ldcv(&poly_psw.xcrd_ovrf[global_read_idx]));
        sh_xfrc[pos]          = x_update;
        sh_xfrc_overflow[pos] = x_update_ovrf;

        // In double-precision mode, the atomic Lennard-Jones parameter array is held in __shared__
        // memory to best utilize the space (a 64kB allocation must be made for other __shared__
        // memory arrays and cannot be used for other L1 caching, so putting this array in
        // __shared__ is essentially free).
        sh_atom_ljidx[pos] = 0;
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 2 * impt_stride) {
      const int rel_pos = pos - impt_stride;
      if (rel_pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + rel_pos]);
        const llint y_update = __ldcv(&poly_psw.yalt[global_read_idx]);
#  ifdef TCALC_IS_SINGLE
        sh_ycrd[rel_pos] = __ldcv(&poly_psw.ycrd[global_read_idx]);
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 y_tmp = longlongToInt63(y_update);
        sh_yfrc[rel_pos]          = y_tmp.x;
        sh_yfrc_overflow[rel_pos] = y_tmp.y;
#    else
        sh_yfrc[rel_pos]          = __ldca(&poly_psw.yalt[global_read_idx]);
#    endif
#  else
        const size_t write_idx = rel_pos + EXCL_GMEM_OFFSET;
        const int y_update_ovrf = __ldcv(&poly_psw.yalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.ycrd[write_idx], __ldcv(&poly_psw.ycrd[global_read_idx]));
        __stwb(&gmem_r.ycrd_ovrf[write_idx], __ldcv(&poly_psw.ycrd_ovrf[global_read_idx]));
        sh_yfrc[rel_pos]          = y_update;
        sh_yfrc_overflow[rel_pos] = y_update_ovrf;
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 3 * impt_stride) {
      const int rel_pos = pos - (2 * impt_stride);
      if (rel_pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + rel_pos]);
        const llint z_update = __ldcv(&poly_psw.zalt[global_read_idx]);
#  ifdef TCALC_IS_SINGLE
        sh_zcrd[rel_pos] = __ldcv(&poly_psw.zcrd[global_read_idx]);
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 z_tmp = longlongToInt63(z_update);
        sh_zfrc[rel_pos]          = z_tmp.x;
        sh_zfrc_overflow[rel_pos] = z_tmp.y;
#    else
        sh_zfrc[rel_pos]          = __ldca(&poly_psw.zalt[global_read_idx]);
#    endif
#  else
        const size_t write_idx = rel_pos + EXCL_GMEM_OFFSET;
        const int z_update_ovrf = __ldcv(&poly_psw.zalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.zcrd[write_idx], __ldcv(&poly_psw.zcrd[global_read_idx]));
        __stwb(&gmem_r.zcrd_ovrf[write_idx], __ldcv(&poly_psw.zcrd_ovrf[global_read_idx]));
        sh_zfrc[rel_pos]          = z_update;
        sh_zfrc_overflow[rel_pos] = z_update_ovrf;
#  endif
      }
      pos += blockDim.x;
    }
    __syncthreads();
#else  // CONSTRAINT_STANDALONE

    // The positions update will have placed the developing particle locations in the force arrays
    // in __shared__ memory, where they are most accessible for iterative refinement.  Thread
    // synchronization will have already been applied.  However, one more organizational step is
    // critical: the Lennard-Jones index array, which is no longer needed by the time particles
    // have been moved, must be repurposed to record whether particles are part of a constraint of
    // any sort.  To prepare for this, its elements for each atom were re-initialized to zero.  The
    // elements will be set to one if the corresponding atoms are part of a constraint.

#endif // CONSTRAINT_STANDALONE
    
    pos = ((threadIdx.x >> warp_bits) << warp_bits);
    const int cgrp_lane_idx = (threadIdx.x & warp_bits_mask_int);
#ifdef CONSTRAINT_STANDALONE
    int vterm_limit = vwu_padded_task_count[(size_t)(VwuAbstractMap::CGROUP)];
#else
    vterm_limit = vwu_padded_task_count[(size_t)(VwuAbstractMap::CGROUP)];
#endif
    while (pos < vterm_limit) {
      const int task_offset = vwu_map[(size_t)(VwuAbstractMap::CGROUP)].x;
      uint2 tinsr;
      if (pos + cgrp_lane_idx < vwu_task_count[(size_t)(VwuAbstractMap::CGROUP)]) {
        tinsr = __ldcv(&poly_auk.cnst_insr[task_offset + pos + cgrp_lane_idx]);
      }
      else {
        tinsr = { 0U, 0U };
      }
      
      // The warp must remain coalesced in order to shuffle changes to the central atom position
      // between threads working on the same group.
      TCALC dx_ref, dy_ref, dz_ref;
      int central_atom = (tinsr.x & 0x3ff);
      int peripheral_atom = ((tinsr.x >> 10) & 0x3ff);
      if (central_atom != 0 || peripheral_atom != 0) {
#ifdef TCALC_IS_SINGLE
        dx_ref = (TCALC)(sh_xcrd[peripheral_atom] - sh_xcrd[central_atom]) *
                 poly_psw.inv_gpos_scale_f;
        dy_ref = (TCALC)(sh_ycrd[peripheral_atom] - sh_ycrd[central_atom]) *
                 poly_psw.inv_gpos_scale_f;
        dz_ref = (TCALC)(sh_zcrd[peripheral_atom] - sh_zcrd[central_atom]) *
                 poly_psw.inv_gpos_scale_f;
#else  // TCALC_IS_SINGLE
        central_atom += EXCL_GMEM_OFFSET;
        peripheral_atom += EXCL_GMEM_OFFSET;
        const int95_t idx_ref = int95Sum(__ldca(&gmem_r.xcrd[peripheral_atom]),
                                         __ldca(&gmem_r.xcrd_ovrf[peripheral_atom]),
                                         -__ldca(&gmem_r.xcrd[central_atom]),
                                         -__ldca(&gmem_r.xcrd_ovrf[central_atom]));
        const int95_t idy_ref = int95Sum(__ldca(&gmem_r.ycrd[peripheral_atom]),
                                         __ldca(&gmem_r.ycrd_ovrf[peripheral_atom]),
                                         -__ldca(&gmem_r.ycrd[central_atom]),
                                         -__ldca(&gmem_r.ycrd_ovrf[central_atom]));
        const int95_t idz_ref = int95Sum(__ldca(&gmem_r.zcrd[peripheral_atom]),
                                         __ldca(&gmem_r.zcrd_ovrf[peripheral_atom]),
                                         -__ldca(&gmem_r.zcrd[central_atom]),
                                         -__ldca(&gmem_r.zcrd_ovrf[central_atom]));
        central_atom -= EXCL_GMEM_OFFSET;
        peripheral_atom -= EXCL_GMEM_OFFSET;
        dx_ref = splitFPToReal(idx_ref) * poly_psw.inv_gpos_scale_f;
        dy_ref = splitFPToReal(idy_ref) * poly_psw.inv_gpos_scale_f;
        dz_ref = splitFPToReal(idz_ref) * poly_psw.inv_gpos_scale_f;
#endif // TCALC_IS_SINGLE

        // Having computed the reference positional delta, the "old" positions of particles are no
        // longer relevant.  Store the positions after the naive update in the "reference"
        // positions array, whether in __shared__ or the thread-block specific cache space.
#ifdef TCALC_IS_SINGLE
        __stwb(&gmem_r.lj_idx[peripheral_atom + EXCL_GMEM_OFFSET], 1);
#  ifdef SPLIT_FORCE_ACCUMULATION
        const llint ph_xcurr_tmp = int63ToLongLong(sh_xfrc[peripheral_atom],
                                                   sh_xfrc_overflow[peripheral_atom]);
        const llint ph_ycurr_tmp = int63ToLongLong(sh_yfrc[peripheral_atom],
                                                   sh_yfrc_overflow[peripheral_atom]);
        const llint ph_zcurr_tmp = int63ToLongLong(sh_zfrc[peripheral_atom],
                                                   sh_zfrc_overflow[peripheral_atom]);
        sh_xcrd[peripheral_atom] = ph_xcurr_tmp;
        sh_ycrd[peripheral_atom] = ph_ycurr_tmp;
        sh_zcrd[peripheral_atom] = ph_zcurr_tmp;
#  else
        sh_xcrd[peripheral_atom] = sh_xfrc[peripheral_atom];
        sh_ycrd[peripheral_atom] = sh_yfrc[peripheral_atom];
        sh_zcrd[peripheral_atom] = sh_zfrc[peripheral_atom];
#  endif
#else
        sh_atom_ljidx[peripheral_atom] = 1;
        const size_t ph_write_idx = peripheral_atom + EXCL_GMEM_OFFSET;
        __stwb(&gmem_r.xcrd[ph_write_idx], sh_xfrc[peripheral_atom]);
        __stwb(&gmem_r.ycrd[ph_write_idx], sh_yfrc[peripheral_atom]);
        __stwb(&gmem_r.zcrd[ph_write_idx], sh_zfrc[peripheral_atom]);
        __stwb(&gmem_r.xcrd_ovrf[ph_write_idx], sh_xfrc_overflow[peripheral_atom]);
        __stwb(&gmem_r.ycrd_ovrf[ph_write_idx], sh_yfrc_overflow[peripheral_atom]);
        __stwb(&gmem_r.zcrd_ovrf[ph_write_idx], sh_zfrc_overflow[peripheral_atom]);
#endif
        // The lead thread in each group will log the central atom's position
        if ((threadIdx.x & warp_bits_mask_int) == ((tinsr.x >> 20) & 0xff)) {
#ifdef TCALC_IS_SINGLE
          __stwb(&gmem_r.lj_idx[central_atom + EXCL_GMEM_OFFSET], 1);
#  ifdef SPLIT_FORCE_ACCUMULATION
          const llint ca_xcurr_tmp = int63ToLongLong(sh_xfrc[central_atom],
                                                     sh_xfrc_overflow[central_atom]);
          const llint ca_ycurr_tmp = int63ToLongLong(sh_yfrc[central_atom],
                                                     sh_yfrc_overflow[central_atom]);
          const llint ca_zcurr_tmp = int63ToLongLong(sh_zfrc[central_atom],
                                                     sh_zfrc_overflow[central_atom]);
          sh_xcrd[central_atom] = ca_xcurr_tmp;
          sh_ycrd[central_atom] = ca_ycurr_tmp;
          sh_zcrd[central_atom] = ca_zcurr_tmp;
#  else
          sh_xcrd[central_atom] = sh_xfrc[central_atom];
          sh_ycrd[central_atom] = sh_yfrc[central_atom];
          sh_zcrd[central_atom] = sh_zfrc[central_atom];
#  endif
#else
          sh_atom_ljidx[central_atom] = 1;
          const size_t ca_write_idx = central_atom + EXCL_GMEM_OFFSET;
          __stwb(&gmem_r.xcrd[ca_write_idx], sh_xfrc[central_atom]);
          __stwb(&gmem_r.ycrd[ca_write_idx], sh_yfrc[central_atom]);
          __stwb(&gmem_r.zcrd[ca_write_idx], sh_zfrc[central_atom]);
          __stwb(&gmem_r.xcrd_ovrf[ca_write_idx], sh_xfrc_overflow[central_atom]);
          __stwb(&gmem_r.ycrd_ovrf[ca_write_idx], sh_yfrc_overflow[central_atom]);
          __stwb(&gmem_r.zcrd_ovrf[ca_write_idx], sh_zfrc_overflow[central_atom]);
#endif
        }
      }
      else {
        dx_ref = (TCALC)(0.0);
        dy_ref = (TCALC)(0.0);
        dz_ref = (TCALC)(0.0);
      }

      const size_t ca_gbl_idx = __ldca(&poly_vk.vwu_imports[impt_llim + central_atom]);
      const size_t ph_gbl_idx = __ldca(&poly_vk.vwu_imports[impt_llim + peripheral_atom]);
      const TCALC ca_invmass = poly_auk.inv_masses[ca_gbl_idx] * poly_psw.gpos_scale_f;
      const TCALC ph_invmass = poly_auk.inv_masses[ph_gbl_idx] * poly_psw.gpos_scale_f;
      const TCALC cbm_factor = (TCALC)(0.6) / poly_auk.cnst_grp_params[tinsr.y].y;

      // "Blank" instructions will be counted as naturally converged, as the positional delta
      // between atoms 0 and 0 will be zero.
      bool converged = false;
      int iter = 0;
      while ((! converged) && iter < tstw.rattle_iter) {
        converged = true;
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
        const int2 idx = int63Sum(sh_xfrc[peripheral_atom], sh_xfrc_overflow[peripheral_atom],
                                  -sh_xfrc[central_atom], -sh_xfrc_overflow[central_atom]);
        const int2 idy = int63Sum(sh_yfrc[peripheral_atom], sh_yfrc_overflow[peripheral_atom],
                                  -sh_yfrc[central_atom], -sh_yfrc_overflow[central_atom]);
        const int2 idz = int63Sum(sh_zfrc[peripheral_atom], sh_zfrc_overflow[peripheral_atom],
                                  -sh_zfrc[central_atom], -sh_zfrc_overflow[central_atom]);
#  else
        const int95_t idx = int95Sum(sh_xfrc[peripheral_atom], sh_xfrc_overflow[peripheral_atom],
                                     -sh_xfrc[central_atom], -sh_xfrc_overflow[central_atom]);
        const int95_t idy = int95Sum(sh_yfrc[peripheral_atom], sh_yfrc_overflow[peripheral_atom],
                                     -sh_yfrc[central_atom], -sh_yfrc_overflow[central_atom]);
        const int95_t idz = int95Sum(sh_zfrc[peripheral_atom], sh_zfrc_overflow[peripheral_atom],
                                     -sh_zfrc[central_atom], -sh_zfrc_overflow[central_atom]);
#  endif
        const TCALC dx = splitFPToReal(idx) * poly_psw.inv_gpos_scale_f;
        const TCALC dy = splitFPToReal(idy) * poly_psw.inv_gpos_scale_f;
        const TCALC dz = splitFPToReal(idz) * poly_psw.inv_gpos_scale_f;
#else  // SPLIT_FORCE_ACCUMULATION
        const TCALC dx = (TCALC)(sh_xfrc[peripheral_atom] - sh_xfrc[central_atom]) *
                         poly_psw.inv_gpos_scale_f;
        const TCALC dy = (TCALC)(sh_yfrc[peripheral_atom] - sh_yfrc[central_atom]) *
                         poly_psw.inv_gpos_scale_f;
        const TCALC dz = (TCALC)(sh_zfrc[peripheral_atom] - sh_zfrc[central_atom]) *
                         poly_psw.inv_gpos_scale_f;
#endif // SPLIT_FORCE_ACCUMULATION
        const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
        const TCALC delta = poly_auk.cnst_grp_params[tinsr.y].x - r2;
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
        int2 cax_update, cay_update, caz_update;
#  else
        llint cax_update, cay_update, caz_update;
#  endif
#else
        int95_t cax_update, cay_update, caz_update;
#endif
        if ((central_atom != 0 || peripheral_atom != 0) && ABS_FUNC(delta) > tstw.rattle_tol) {
          converged = false;
          const TCALC dot_disp = (dx * dx_ref) + (dy * dy_ref) + (dz * dz_ref);
          const TCALC term = cbm_factor * delta / dot_disp;
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
          // The position scaling factor was folded into the inverse mass quantity (see above)
          const int2 phx_update = floatToInt63(dx_ref * term * ph_invmass);
          const int2 phy_update = floatToInt63(dy_ref * term * ph_invmass);
          const int2 phz_update = floatToInt63(dz_ref * term * ph_invmass);
          const int2 nph_x = splitFPSum(phx_update, sh_xfrc[peripheral_atom],
                                        sh_xfrc_overflow[peripheral_atom]);
          const int2 nph_y = splitFPSum(phy_update, sh_yfrc[peripheral_atom],
                                        sh_yfrc_overflow[peripheral_atom]);
          const int2 nph_z = splitFPSum(phz_update, sh_zfrc[peripheral_atom],
                                        sh_zfrc_overflow[peripheral_atom]);
          cax_update = floatToInt63(-dx_ref * term * ca_invmass);
          cay_update = floatToInt63(-dy_ref * term * ca_invmass);
          caz_update = floatToInt63(-dz_ref * term * ca_invmass);
#  else
          const int95_t phx_update = doubleToInt95(dx_ref * term * ph_invmass);
          const int95_t phy_update = doubleToInt95(dy_ref * term * ph_invmass);
          const int95_t phz_update = doubleToInt95(dz_ref * term * ph_invmass);
          const int95_t nph_x = splitFPSum(phx_update, sh_xfrc[peripheral_atom],
                                           sh_xfrc_overflow[peripheral_atom]);
          const int95_t nph_y = splitFPSum(phy_update, sh_yfrc[peripheral_atom],
                                           sh_yfrc_overflow[peripheral_atom]);
          const int95_t nph_z = splitFPSum(phz_update, sh_zfrc[peripheral_atom],
                                           sh_zfrc_overflow[peripheral_atom]);
          cax_update = doubleToInt95(-dx_ref * term * ca_invmass);
          cay_update = doubleToInt95(-dy_ref * term * ca_invmass);
          caz_update = doubleToInt95(-dz_ref * term * ca_invmass);
#  endif
          sh_xfrc[peripheral_atom] = nph_x.x;
          sh_yfrc[peripheral_atom] = nph_y.x;
          sh_zfrc[peripheral_atom] = nph_z.x;
          sh_xfrc_overflow[peripheral_atom] = nph_x.y;
          sh_yfrc_overflow[peripheral_atom] = nph_y.y;
          sh_zfrc_overflow[peripheral_atom] = nph_z.y;
#else
          sh_xfrc[peripheral_atom] += LLCONV_FUNC(dx_ref * term * ph_invmass);
          sh_yfrc[peripheral_atom] += LLCONV_FUNC(dy_ref * term * ph_invmass);
          sh_zfrc[peripheral_atom] += LLCONV_FUNC(dz_ref * term * ph_invmass);
          cax_update = LLCONV_FUNC(-dx_ref * term * ca_invmass);
          cay_update = LLCONV_FUNC(-dy_ref * term * ca_invmass);
          caz_update = LLCONV_FUNC(-dz_ref * term * ca_invmass);
#endif
        }
        else {
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
          cax_update = { 0, 0 };
          cay_update = { 0, 0 };
          caz_update = { 0, 0 };
#  else
          cax_update = { 0LL, 0 };
          cay_update = { 0LL, 0 };
          caz_update = { 0LL, 0 };
#  endif
#else
          cax_update = 0LL;
          cay_update = 0LL;
          caz_update = 0LL;
#endif
        }
        
        // The peripheral atom positions may be left as they are, if the calculation has converged.
        // Reduce the moves on the central atoms.
        const bool leader_lane = (cgrp_lane_idx == ((tinsr.x >> 20) & 0xff));
        const int spoke_count = (tinsr.x >> 28);
        for (int i = 1; i < poly_auk.largest_group; i++) {
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
          int2 ncax, ncay, ncaz;
#  else
          int95_t ncax, ncay, ncaz;
#  endif
          ncax.x = SHFL(cax_update.x, cgrp_lane_idx + i);
          ncax.y = SHFL(cax_update.y, cgrp_lane_idx + i);
          ncay.x = SHFL(cay_update.x, cgrp_lane_idx + i);
          ncay.y = SHFL(cay_update.y, cgrp_lane_idx + i);
          ncaz.x = SHFL(caz_update.x, cgrp_lane_idx + i);
          ncaz.y = SHFL(caz_update.y, cgrp_lane_idx + i);
#else
          const llint ncax = SHFL(cax_update, cgrp_lane_idx + i);
          const llint ncay = SHFL(cay_update, cgrp_lane_idx + i);
          const llint ncaz = SHFL(caz_update, cgrp_lane_idx + i);
#endif
          if (leader_lane && i < spoke_count) {
#ifdef SPLIT_FORCE_ACCUMULATION
            cax_update = splitFPSum(cax_update, ncax);
            cay_update = splitFPSum(cay_update, ncay);
            caz_update = splitFPSum(caz_update, ncaz);
#else
            cax_update += ncax;
            cay_update += ncay;
            caz_update += ncaz;
#endif
          }
        }

        // Check again for blank instructions
        if (central_atom != 0 || peripheral_atom != 0) {

          // The leader lane will update the central atom's position.
          if (leader_lane) {
#ifdef SPLIT_FORCE_ACCUMULATION
            cax_update = splitFPSum(cax_update,
                                    sh_xfrc[central_atom], sh_xfrc_overflow[central_atom]);
            cay_update = splitFPSum(cay_update,
                                    sh_yfrc[central_atom], sh_yfrc_overflow[central_atom]);
            caz_update = splitFPSum(caz_update,
                                    sh_zfrc[central_atom], sh_zfrc_overflow[central_atom]);
            sh_xfrc[central_atom] = cax_update.x;
            sh_yfrc[central_atom] = cay_update.x;
            sh_zfrc[central_atom] = caz_update.x;
            sh_xfrc_overflow[central_atom] = cax_update.y;
            sh_yfrc_overflow[central_atom] = cay_update.y;
            sh_zfrc_overflow[central_atom] = caz_update.y;
#else
            sh_xfrc[central_atom] += cax_update;
            sh_yfrc[central_atom] += cay_update;
            sh_zfrc[central_atom] += caz_update;
#endif
          }
        }

        // Test convergence across the whole warp.  The implicit warp synchronization in this
        // call will also ensure that the __shared__ memory array updates are ready if convergence
        // is not complete.
        converged = (BALLOT(! converged) == 0x0);
        iter++;
      }
      pos += blockDim.x;
    }

    // The updated positions are now in the force arrays.  In the context of a more extensive
    // valence kernel, the positions are in the proper memory space to guide virtual site
    // placement, if requested, and ultimately to be flushed back to the global positions arrays.
    // In the context of a standalone kernel, the positions should be returned to arrays in the
    // coordinate synthesis now.
    __syncthreads();
    pos = threadIdx.x;
    const TCALC cfac = poly_psw.inv_gpos_scale_f * poly_psw.vel_scale_f / tstw.dt;        
    while (pos < impt_count) {
      const int2 mask_limits =
        poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                              static_cast<int>(VwuAbstractMap::MANIPULATE)];
      const int pos_updt_elem = (pos >> 5);
      const int pos_updt_bit  = pos - (pos_updt_elem << 5);
      const uint2 permit = __ldca(&poly_auk.vwu_manip[mask_limits.x + pos_updt_elem]);
      if ((permit.y >> pos_updt_bit) & 0x1) {
        const size_t global_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);

        // Write the new positions to the appropriate arrays, if this is a stand-alone kernel.
        // Otherwise, wait for possible virtual site placements and write positions at the end
        // of the valence work unit kernel.
#ifdef CONSTRAINT_STANDALONE
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
        const llint x_tmp = int63ToLongLong(sh_xfrc[pos], sh_xfrc_overflow[pos]);
        const llint y_tmp = int63ToLongLong(sh_yfrc[pos], sh_yfrc_overflow[pos]);
        const llint z_tmp = int63ToLongLong(sh_zfrc[pos], sh_zfrc_overflow[pos]);
        __stwb(&poly_psw.xalt[global_idx], x_tmp);
        __stwb(&poly_psw.yalt[global_idx], y_tmp);
        __stwb(&poly_psw.zalt[global_idx], z_tmp);
#    else
        __stwb(&poly_psw.xalt[global_idx], sh_xfrc[pos]);
        __stwb(&poly_psw.yalt[global_idx], sh_yfrc[pos]);
        __stwb(&poly_psw.zalt[global_idx], sh_zfrc[pos]);
#    endif
#  else
        __stwb(&poly_psw.xalt[global_idx], sh_xfrc[pos]);
        __stwb(&poly_psw.yalt[global_idx], sh_yfrc[pos]);
        __stwb(&poly_psw.zalt[global_idx], sh_zfrc[pos]);
        __stwb(&poly_psw.xalt_ovrf[global_idx], sh_xfrc_overflow[pos]);
        __stwb(&poly_psw.yalt_ovrf[global_idx], sh_yfrc_overflow[pos]);
        __stwb(&poly_psw.zalt_ovrf[global_idx], sh_zfrc_overflow[pos]);
#  endif
#endif
        // Compute the velocity adjustments due to geometric constraints.  The Lennard-Jones
        // parameter array has been refilled with information on whether constraints acted on
        // any given atom.
        const size_t local_idx = pos + EXCL_GMEM_OFFSET;
#ifdef TCALC_IS_SINGLE
        const bool particle_was_constrained = (__ldca(&gmem_r.lj_idx[local_idx]) == 1);
#else
        const bool particle_was_constrained = (sh_atom_ljidx[pos] == 1);
#endif
        TCALC gc_dx, gc_dy, gc_dz;
        if (particle_was_constrained) {
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
#    ifndef CONSTRAINT_STANDALONE
          const llint x_tmp = int63ToLongLong(sh_xfrc[pos], sh_xfrc_overflow[pos]);
          const llint y_tmp = int63ToLongLong(sh_yfrc[pos], sh_yfrc_overflow[pos]);
          const llint z_tmp = int63ToLongLong(sh_zfrc[pos], sh_zfrc_overflow[pos]);
#    endif
          gc_dx = (TCALC)(x_tmp - sh_xcrd[pos]) * cfac;
          gc_dy = (TCALC)(y_tmp - sh_ycrd[pos]) * cfac;
          gc_dz = (TCALC)(z_tmp - sh_zcrd[pos]) * cfac;
#  else
          gc_dx = (TCALC)(sh_xfrc[pos] - sh_xcrd[pos]) * cfac;
          gc_dy = (TCALC)(sh_yfrc[pos] - sh_ycrd[pos]) * cfac;
          gc_dz = (TCALC)(sh_zfrc[pos] - sh_zcrd[pos]) * cfac;
#  endif
#else  // TCALC_IS_SINGLE
          gc_dx = int95SumToDouble(sh_xfrc[pos], sh_xfrc_overflow[pos],
                                   -__ldca(&gmem_r.xcrd[local_idx]),
                                   -__ldca(&gmem_r.xcrd_ovrf[local_idx])) * cfac;
          gc_dy = int95SumToDouble(sh_yfrc[pos], sh_yfrc_overflow[pos],
                                   -__ldca(&gmem_r.ycrd[local_idx]),
                                   -__ldca(&gmem_r.ycrd_ovrf[local_idx])) * cfac;
          gc_dz = int95SumToDouble(sh_zfrc[pos], sh_zfrc_overflow[pos],
                                   -__ldca(&gmem_r.zcrd[local_idx]),
                                   -__ldca(&gmem_r.zcrd_ovrf[local_idx])) * cfac;
#endif // TCALC_IS_SINGLE
        }
        else {
          gc_dx = (TCALC)(0.0);
          gc_dy = (TCALC)(0.0);
          gc_dz = (TCALC)(0.0);
        }

        // The adjustments to velocities have been computed, but a standalone kernel will draw
        // the velocities themselves from global arrays in the developmental half of the
        // coordinate synthesis, whereas if the code is embedded within a valence work unit the
        // velocities will have been developed from the beginning of integration process and
        // stored in the thread block's private, local resources.  Once adjusted, the velocities
        // will be written back to the global arrays in the coordinate synthesis.
#ifdef TCALC_IS_SINGLE
#  ifdef CONSTRAINT_STANDALONE
        const llint adj_xvel = __ldlu(&poly_psw.vxalt[global_idx]) + LLCONV_FUNC(gc_dx);
        const llint adj_yvel = __ldlu(&poly_psw.vyalt[global_idx]) + LLCONV_FUNC(gc_dy);
        const llint adj_zvel = __ldlu(&poly_psw.vzalt[global_idx]) + LLCONV_FUNC(gc_dz);
#  else
        const llint adj_xvel = __ldca(&gmem_r.xvel[local_idx]) + LLCONV_FUNC(gc_dx);
        const llint adj_yvel = __ldca(&gmem_r.yvel[local_idx]) + LLCONV_FUNC(gc_dy);
        const llint adj_zvel = __ldca(&gmem_r.zvel[local_idx]) + LLCONV_FUNC(gc_dz);
#  endif
        __stwt(&poly_psw.vxalt[global_idx], adj_xvel);
        __stwt(&poly_psw.vyalt[global_idx], adj_yvel);
        __stwt(&poly_psw.vzalt[global_idx], adj_zvel);
#else  // TCALC_IS_SINGLE
#  ifdef CONSTRAINT_STANDALONE
        const int95_t adj_xvel = int95Sum(__ldcv(&poly_psw.vxalt[global_idx]),
                                          __ldcv(&poly_psw.vxalt_ovrf[global_idx]), gc_dx);
        const int95_t adj_yvel = int95Sum(__ldcv(&poly_psw.vyalt[global_idx]),
                                          __ldcv(&poly_psw.vyalt_ovrf[global_idx]), gc_dy);
        const int95_t adj_zvel = int95Sum(__ldcv(&poly_psw.vzalt[global_idx]),
                                          __ldcv(&poly_psw.vzalt_ovrf[global_idx]), gc_dz);
#  else
        const int95_t adj_xvel = int95Sum(__ldcv(&gmem_r.xvel[local_idx]),
                                          __ldcv(&gmem_r.xvel_ovrf[local_idx]), gc_dx);
        const int95_t adj_yvel = int95Sum(__ldcv(&gmem_r.yvel[local_idx]),
                                          __ldcv(&gmem_r.yvel_ovrf[local_idx]), gc_dy);
        const int95_t adj_zvel = int95Sum(__ldcv(&gmem_r.zvel[local_idx]),
                                          __ldcv(&gmem_r.zvel_ovrf[local_idx]), gc_dz);
#  endif
        __stwb(&poly_psw.vxalt[global_idx], adj_xvel.x);
        __stwb(&poly_psw.vyalt[global_idx], adj_yvel.x);
        __stwb(&poly_psw.vzalt[global_idx], adj_zvel.x);
        __stwb(&poly_psw.vxalt_ovrf[global_idx], adj_xvel.y);
        __stwb(&poly_psw.vyalt_ovrf[global_idx], adj_yvel.y);
        __stwb(&poly_psw.vzalt_ovrf[global_idx], adj_zvel.y);
#endif // TCALC_IS_SINGLE
      }
      pos += blockDim.x;
    }
#ifdef CONSTRAINT_STANDALONE
    // Proceed to the next valence work unit.
    __syncthreads();
    if (threadIdx.x == 0) {
      const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
      vwu_idx = atomicAdd(&ctrl.gcns_progress[prog_counter_idx], 1);
    }
    __syncthreads();
  } // Close the loop over all valence work units

  // Set the block counters for future iterations of this kernel
  if (blockIdx.x == 0 && threadIdx.x < warp_size_int) {
    const int step_modulus = (ctrl.step & twice_warp_bits_mask_int);
    if (step_modulus == 0) {
      ctrl.gcns_progress[threadIdx.x + warp_size_int] = gridDim.x;
    }
    if (step_modulus == warp_size_int) {
      ctrl.gcns_progress[threadIdx.x] = gridDim.x;
    }
  }
}

// Clear definitions of the valence atom capacity and the offset into the thread-block exclusive
// cache space.
#  undef VALENCE_ATOM_CAPACITY
#  undef EXCL_GMEM_OFFSET
#endif // CONSTRAINT_STANDALONE
